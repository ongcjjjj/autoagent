# è‡ªä¸»è¿›åŒ–Agent - ç¬¬1è½®æå‡ï¼šé«˜çº§æ¨ç†å¼•æ“2.0
# Advanced Reasoning Engine v2.0 - æ··åˆç¬¦å·ä¸ç¥ç»æ¨ç†æ¶æ„

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Tuple, Any, Optional, Union
import networkx as nx
from dataclasses import dataclass
from enum import Enum
import json
import asyncio
from collections import defaultdict
import sympy as sp
from transformers import AutoTokenizer, AutoModel
import logging
from abc import ABC, abstractmethod

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ReasoningType(Enum):
    """æ¨ç†ç±»å‹æšä¸¾"""
    DEDUCTIVE = "deductive"      # æ¼”ç»æ¨ç†
    INDUCTIVE = "inductive"      # å½’çº³æ¨ç†
    ABDUCTIVE = "abductive"      # æº¯å› æ¨ç†
    ANALOGICAL = "analogical"    # ç±»æ¯”æ¨ç†
    CAUSAL = "causal"           # å› æœæ¨ç†
    TEMPORAL = "temporal"       # æ—¶åºæ¨ç†
    SPATIAL = "spatial"         # ç©ºé—´æ¨ç†
    MODAL = "modal"             # æ¨¡æ€æ¨ç†

@dataclass
class ReasoningPremise:
    """æ¨ç†å‰ææ•°æ®ç»“æ„"""
    content: str
    confidence: float
    type: str
    source: str
    timestamp: float
    metadata: Dict[str, Any] = None

@dataclass
class ReasoningConclusion:
    """æ¨ç†ç»“è®ºæ•°æ®ç»“æ„"""
    content: str
    confidence: float
    reasoning_type: ReasoningType
    premises: List[ReasoningPremise]
    reasoning_path: List[str]
    explanation: str
    validity_score: float

class SymbolicReasoningEngine:
    """ç¬¦å·æ¨ç†å¼•æ“"""
    
    def __init__(self):
        self.knowledge_base = nx.DiGraph()
        self.rules = []
        self.facts = set()
        self.inference_chains = []
        
    def add_fact(self, fact: str, confidence: float = 1.0):
        """æ·»åŠ äº‹å®åˆ°çŸ¥è¯†åº“"""
        self.facts.add(fact)
        self.knowledge_base.add_node(fact, type='fact', confidence=confidence)
        logger.info(f"æ·»åŠ äº‹å®: {fact} (ç½®ä¿¡åº¦: {confidence})")
        
    def add_rule(self, premise: str, conclusion: str, rule_type: str = "implication"):
        """æ·»åŠ æ¨ç†è§„åˆ™"""
        rule = {
            'premise': premise,
            'conclusion': conclusion,
            'type': rule_type,
            'strength': 1.0
        }
        self.rules.append(rule)
        self.knowledge_base.add_edge(premise, conclusion, rule=rule)
        logger.info(f"æ·»åŠ è§„åˆ™: {premise} -> {conclusion}")
        
    def forward_chaining(self, query: str) -> List[ReasoningConclusion]:
        """å‰å‘é“¾å¼æ¨ç†"""
        conclusions = []
        visited = set()
        
        def chain_forward(current_facts):
            new_facts = set()
            for rule in self.rules:
                if rule['premise'] in current_facts and rule['premise'] not in visited:
                    visited.add(rule['premise'])
                    conclusion = rule['conclusion']
                    new_facts.add(conclusion)
                    
                    # è®¡ç®—æ¨ç†ç½®ä¿¡åº¦
                    premise_confidence = self.knowledge_base.nodes.get(rule['premise'], {}).get('confidence', 1.0)
                    conclusion_confidence = premise_confidence * rule['strength'] * 0.9
                    
                    reasoning_conclusion = ReasoningConclusion(
                        content=conclusion,
                        confidence=conclusion_confidence,
                        reasoning_type=ReasoningType.DEDUCTIVE,
                        premises=[ReasoningPremise(rule['premise'], premise_confidence, 'fact', 'KB', 0.0)],
                        reasoning_path=[rule['premise'], conclusion],
                        explanation=f"åŸºäºè§„åˆ™ '{rule['premise']} -> {conclusion}' è¿›è¡Œæ¼”ç»æ¨ç†",
                        validity_score=conclusion_confidence
                    )
                    conclusions.append(reasoning_conclusion)
                    
            if new_facts:
                current_facts.update(new_facts)
                chain_forward(current_facts)
                
        chain_forward(self.facts.copy())
        return conclusions
        
    def backward_chaining(self, goal: str) -> List[ReasoningConclusion]:
        """åå‘é“¾å¼æ¨ç†"""
        conclusions = []
        
        def prove_goal(target, depth=0, max_depth=10):
            if depth > max_depth:
                return False
                
            if target in self.facts:
                return True
                
            for rule in self.rules:
                if rule['conclusion'] == target:
                    if prove_goal(rule['premise'], depth + 1):
                        premise_confidence = self.knowledge_base.nodes.get(rule['premise'], {}).get('confidence', 1.0)
                        conclusion_confidence = premise_confidence * rule['strength'] * 0.85
                        
                        reasoning_conclusion = ReasoningConclusion(
                            content=target,
                            confidence=conclusion_confidence,
                            reasoning_type=ReasoningType.DEDUCTIVE,
                            premises=[ReasoningPremise(rule['premise'], premise_confidence, 'fact', 'KB', 0.0)],
                            reasoning_path=[rule['premise'], target],
                            explanation=f"é€šè¿‡åå‘æ¨ç†è¯æ˜ç›®æ ‡ '{target}'",
                            validity_score=conclusion_confidence
                        )
                        conclusions.append(reasoning_conclusion)
                        return True
            return False
            
        prove_goal(goal)
        return conclusions

class NeuralReasoningModule(nn.Module):
    """ç¥ç»æ¨ç†æ¨¡å—"""
    
    def __init__(self, input_dim: int = 768, hidden_dim: int = 512, num_heads: int = 8):
        super().__init__()
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        
        # å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
        self.multi_head_attention = nn.MultiheadAttention(
            embed_dim=input_dim,
            num_heads=num_heads,
            dropout=0.1
        )
        
        # æ¨ç†ç½‘ç»œ
        self.reasoning_network = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, input_dim)
        )
        
        # ç½®ä¿¡åº¦é¢„æµ‹ç½‘ç»œ
        self.confidence_network = nn.Sequential(
            nn.Linear(input_dim, hidden_dim // 4),
            nn.ReLU(),
            nn.Linear(hidden_dim // 4, 1),
            nn.Sigmoid()
        )
        
        # æ¨ç†ç±»å‹åˆ†ç±»å™¨
        self.reasoning_type_classifier = nn.Sequential(
            nn.Linear(input_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, len(ReasoningType)),
            nn.Softmax(dim=-1)
        )
        
    def forward(self, premises: torch.Tensor, query: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """å‰å‘æ¨ç†"""
        # æ³¨æ„åŠ›æœºåˆ¶å¤„ç†å‰æå’ŒæŸ¥è¯¢
        attended_premises, attention_weights = self.multi_head_attention(
            query.unsqueeze(0), premises, premises
        )
        
        # æ¨ç†è®¡ç®—
        reasoning_output = self.reasoning_network(attended_premises.squeeze(0))
        
        # ç½®ä¿¡åº¦é¢„æµ‹
        confidence = self.confidence_network(reasoning_output)
        
        # æ¨ç†ç±»å‹é¢„æµ‹
        reasoning_type_probs = self.reasoning_type_classifier(reasoning_output)
        
        return reasoning_output, confidence, reasoning_type_probs

class CausalReasoningEngine:
    """å› æœæ¨ç†å¼•æ“"""
    
    def __init__(self):
        self.causal_graph = nx.DiGraph()
        self.interventions = {}
        
    def add_causal_relation(self, cause: str, effect: str, strength: float = 1.0):
        """æ·»åŠ å› æœå…³ç³»"""
        self.causal_graph.add_edge(cause, effect, strength=strength)
        
    def find_causal_path(self, cause: str, effect: str) -> List[str]:
        """å¯»æ‰¾å› æœè·¯å¾„"""
        try:
            return nx.shortest_path(self.causal_graph, cause, effect)
        except nx.NetworkXNoPath:
            return []
            
    def counterfactual_reasoning(self, intervention: Dict[str, Any], target: str) -> float:
        """åäº‹å®æ¨ç†"""
        # è®¡ç®—å¹²é¢„å¯¹ç›®æ ‡çš„å½±å“
        original_value = self.causal_graph.nodes.get(target, {}).get('value', 0.0)
        
        # æ¨¡æ‹Ÿå¹²é¢„
        modified_graph = self.causal_graph.copy()
        for node, value in intervention.items():
            if node in modified_graph:
                modified_graph.nodes[node]['value'] = value
                
        # è®¡ç®—æ–°çš„ç›®æ ‡å€¼ï¼ˆç®€åŒ–è®¡ç®—ï¼‰
        affected_paths = []
        for intervention_node in intervention:
            if nx.has_path(modified_graph, intervention_node, target):
                path = nx.shortest_path(modified_graph, intervention_node, target)
                affected_paths.append(path)
                
        # è®¡ç®—å½±å“å¼ºåº¦
        total_effect = 0.0
        for path in affected_paths:
            path_strength = 1.0
            for i in range(len(path) - 1):
                edge_data = modified_graph.get_edge_data(path[i], path[i+1])
                if edge_data:
                    path_strength *= edge_data.get('strength', 1.0)
            total_effect += path_strength
            
        return min(abs(total_effect), 1.0)

class AnalogicalReasoningEngine:
    """ç±»æ¯”æ¨ç†å¼•æ“"""
    
    def __init__(self):
        self.analogies = []
        self.structure_mappings = {}
        
    def add_analogy(self, source: Dict[str, Any], target: Dict[str, Any], mapping: Dict[str, str]):
        """æ·»åŠ ç±»æ¯”æ˜ å°„"""
        analogy = {
            'source': source,
            'target': target,
            'mapping': mapping,
            'similarity': self.compute_similarity(source, target, mapping)
        }
        self.analogies.append(analogy)
        
    def compute_similarity(self, source: Dict[str, Any], target: Dict[str, Any], mapping: Dict[str, str]) -> float:
        """è®¡ç®—ç»“æ„ç›¸ä¼¼åº¦"""
        total_matches = 0
        total_elements = len(mapping)
        
        for source_elem, target_elem in mapping.items():
            if source_elem in source and target_elem in target:
                source_type = type(source[source_elem])
                target_type = type(target[target_elem])
                if source_type == target_type:
                    total_matches += 1
                    
        return total_matches / total_elements if total_elements > 0 else 0.0
        
    def analogical_inference(self, source_domain: str, target_domain: str, query: str) -> List[ReasoningConclusion]:
        """ç±»æ¯”æ¨ç†"""
        conclusions = []
        
        for analogy in self.analogies:
            if analogy['similarity'] > 0.7:  # é«˜ç›¸ä¼¼åº¦é˜ˆå€¼
                # åŸºäºç±»æ¯”ç”Ÿæˆæ¨ç†
                mapped_conclusion = self.apply_mapping(query, analogy['mapping'])
                
                conclusion = ReasoningConclusion(
                    content=mapped_conclusion,
                    confidence=analogy['similarity'] * 0.8,
                    reasoning_type=ReasoningType.ANALOGICAL,
                    premises=[ReasoningPremise(query, 1.0, 'query', 'user', 0.0)],
                    reasoning_path=[source_domain, target_domain, mapped_conclusion],
                    explanation=f"åŸºäº{source_domain}å’Œ{target_domain}çš„ç±»æ¯”æ¨ç†",
                    validity_score=analogy['similarity']
                )
                conclusions.append(conclusion)
                
        return conclusions
        
    def apply_mapping(self, content: str, mapping: Dict[str, str]) -> str:
        """åº”ç”¨æ˜ å°„è½¬æ¢"""
        result = content
        for source_elem, target_elem in mapping.items():
            result = result.replace(source_elem, target_elem)
        return result

class HybridReasoningEngine:
    """æ··åˆæ¨ç†å¼•æ“ - æ•´åˆç¬¦å·ä¸ç¥ç»æ¨ç†"""
    
    def __init__(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2"):
        # åˆå§‹åŒ–å„ä¸ªæ¨ç†ç»„ä»¶
        self.symbolic_engine = SymbolicReasoningEngine()
        self.neural_module = NeuralReasoningModule()
        self.causal_engine = CausalReasoningEngine()
        self.analogical_engine = AnalogicalReasoningEngine()
        
        # æ–‡æœ¬ç¼–ç å™¨
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.encoder = AutoModel.from_pretrained(model_name)
        
        # æ¨ç†å†å²
        self.reasoning_history = []
        self.performance_metrics = {
            'total_inferences': 0,
            'successful_inferences': 0,
            'avg_confidence': 0.0,
            'reasoning_type_distribution': defaultdict(int)
        }
        
    def encode_text(self, text: str) -> torch.Tensor:
        """ç¼–ç æ–‡æœ¬ä¸ºå‘é‡è¡¨ç¤º"""
        inputs = self.tokenizer(text, return_tensors="pt", padding=True, truncation=True)
        with torch.no_grad():
            outputs = self.encoder(**inputs)
            embeddings = outputs.last_hidden_state.mean(dim=1)
        return embeddings
        
    def multi_modal_reasoning(self, query: str, premises: List[str], reasoning_types: List[ReasoningType] = None) -> List[ReasoningConclusion]:
        """å¤šæ¨¡æ€æ¨ç† - ç»¼åˆç¬¦å·ä¸ç¥ç»æ¨ç†"""
        all_conclusions = []
        
        if reasoning_types is None:
            reasoning_types = [ReasoningType.DEDUCTIVE, ReasoningType.INDUCTIVE, ReasoningType.ANALOGICAL]
            
        # 1. ç¬¦å·æ¨ç†
        if ReasoningType.DEDUCTIVE in reasoning_types:
            # æ·»åŠ å‰æåˆ°çŸ¥è¯†åº“
            for premise in premises:
                self.symbolic_engine.add_fact(premise)
                
            # æ‰§è¡Œå‰å‘å’Œåå‘æ¨ç†
            forward_conclusions = self.symbolic_engine.forward_chaining(query)
            backward_conclusions = self.symbolic_engine.backward_chaining(query)
            all_conclusions.extend(forward_conclusions + backward_conclusions)
            
        # 2. ç¥ç»æ¨ç†
        if any(rt in reasoning_types for rt in [ReasoningType.INDUCTIVE, ReasoningType.ABDUCTIVE]):
            # ç¼–ç æ–‡æœ¬
            query_embedding = self.encode_text(query)
            premise_embeddings = torch.stack([self.encode_text(p) for p in premises])
            
            # ç¥ç»æ¨ç†
            reasoning_output, confidence, reasoning_type_probs = self.neural_module(
                premise_embeddings, query_embedding
            )
            
            # è§£ç æ¨ç†ç»“æœ
            neural_conclusion = ReasoningConclusion(
                content=f"ç¥ç»æ¨ç†ç»“æœ: {query}",
                confidence=confidence.item(),
                reasoning_type=ReasoningType.INDUCTIVE,
                premises=[ReasoningPremise(p, 0.9, 'premise', 'user', 0.0) for p in premises],
                reasoning_path=premises + [query],
                explanation="åŸºäºç¥ç»ç½‘ç»œçš„å½’çº³æ¨ç†",
                validity_score=confidence.item()
            )
            all_conclusions.append(neural_conclusion)
            
        # 3. å› æœæ¨ç†
        if ReasoningType.CAUSAL in reasoning_types:
            # æ„å»ºä¸´æ—¶å› æœå›¾
            for i, premise in enumerate(premises):
                if i < len(premises) - 1:
                    self.causal_engine.add_causal_relation(premise, premises[i+1])
                    
            # å¯»æ‰¾å› æœè·¯å¾„
            if len(premises) > 0:
                causal_path = self.causal_engine.find_causal_path(premises[0], query)
                if causal_path:
                    causal_conclusion = ReasoningConclusion(
                        content=f"å› æœæ¨ç†: {' -> '.join(causal_path)}",
                        confidence=0.8,
                        reasoning_type=ReasoningType.CAUSAL,
                        premises=[ReasoningPremise(p, 0.9, 'premise', 'user', 0.0) for p in premises],
                        reasoning_path=causal_path,
                        explanation=f"å‘ç°å› æœé“¾: {' -> '.join(causal_path)}",
                        validity_score=0.8
                    )
                    all_conclusions.append(causal_conclusion)
                    
        # 4. ç±»æ¯”æ¨ç†
        if ReasoningType.ANALOGICAL in reasoning_types:
            analogical_conclusions = self.analogical_engine.analogical_inference(
                "æºåŸŸ", "ç›®æ ‡åŸŸ", query
            )
            all_conclusions.extend(analogical_conclusions)
            
        # 5. ç»“è®ºèåˆä¸æ’åº
        fused_conclusions = self.fuse_conclusions(all_conclusions)
        
        # æ›´æ–°æ¨ç†å†å²å’Œæ€§èƒ½æŒ‡æ ‡
        self.update_metrics(fused_conclusions)
        
        return fused_conclusions
        
    def fuse_conclusions(self, conclusions: List[ReasoningConclusion]) -> List[ReasoningConclusion]:
        """èåˆå¤šä¸ªæ¨ç†ç»“è®º"""
        if not conclusions:
            return []
            
        # æŒ‰ç½®ä¿¡åº¦å’Œæœ‰æ•ˆæ€§å¾—åˆ†æ’åº
        conclusions.sort(key=lambda x: (x.confidence + x.validity_score) / 2, reverse=True)
        
        # å»é‡ç›¸ä¼¼ç»“è®º
        unique_conclusions = []
        seen_contents = set()
        
        for conclusion in conclusions:
            if conclusion.content not in seen_contents:
                unique_conclusions.append(conclusion)
                seen_contents.add(conclusion.content)
                
        return unique_conclusions[:10]  # è¿”å›Top 10ç»“è®º
        
    def update_metrics(self, conclusions: List[ReasoningConclusion]):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
        self.performance_metrics['total_inferences'] += 1
        
        if conclusions:
            self.performance_metrics['successful_inferences'] += 1
            avg_confidence = sum(c.confidence for c in conclusions) / len(conclusions)
            self.performance_metrics['avg_confidence'] = (
                self.performance_metrics['avg_confidence'] * (self.performance_metrics['total_inferences'] - 1) +
                avg_confidence
            ) / self.performance_metrics['total_inferences']
            
            for conclusion in conclusions:
                self.performance_metrics['reasoning_type_distribution'][conclusion.reasoning_type.value] += 1
                
    def explain_reasoning(self, conclusion: ReasoningConclusion) -> str:
        """è§£é‡Šæ¨ç†è¿‡ç¨‹"""
        explanation = f"""
æ¨ç†ç»“è®º: {conclusion.content}
æ¨ç†ç±»å‹: {conclusion.reasoning_type.value}
ç½®ä¿¡åº¦: {conclusion.confidence:.3f}
æœ‰æ•ˆæ€§å¾—åˆ†: {conclusion.validity_score:.3f}

æ¨ç†è·¯å¾„:
{' -> '.join(conclusion.reasoning_path)}

è¯¦ç»†è§£é‡Š:
{conclusion.explanation}

ä½¿ç”¨çš„å‰æ:
"""
        for i, premise in enumerate(conclusion.premises, 1):
            explanation += f"{i}. {premise.content} (ç½®ä¿¡åº¦: {premise.confidence:.3f})\n"
            
        return explanation
        
    def get_performance_report(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        success_rate = (
            self.performance_metrics['successful_inferences'] / 
            max(self.performance_metrics['total_inferences'], 1)
        )
        
        return {
            "æ€»æ¨ç†æ¬¡æ•°": self.performance_metrics['total_inferences'],
            "æˆåŠŸæ¨ç†æ¬¡æ•°": self.performance_metrics['successful_inferences'],
            "æˆåŠŸç‡": f"{success_rate:.2%}",
            "å¹³å‡ç½®ä¿¡åº¦": f"{self.performance_metrics['avg_confidence']:.3f}",
            "æ¨ç†ç±»å‹åˆ†å¸ƒ": dict(self.performance_metrics['reasoning_type_distribution']),
            "æ¨ç†å†å²é•¿åº¦": len(self.reasoning_history)
        }

# ç¤ºä¾‹ä½¿ç”¨å’Œæµ‹è¯•
async def demonstrate_hybrid_reasoning():
    """æ¼”ç¤ºæ··åˆæ¨ç†å¼•æ“åŠŸèƒ½"""
    print("ğŸ”¥ è‡ªä¸»è¿›åŒ–Agent - ç¬¬1è½®æå‡ï¼šé«˜çº§æ¨ç†å¼•æ“2.0")
    print("=" * 60)
    
    # åˆ›å»ºæ··åˆæ¨ç†å¼•æ“
    reasoning_engine = HybridReasoningEngine()
    
    # ç¤ºä¾‹1: æ¼”ç»æ¨ç†
    print("\nğŸ“š ç¤ºä¾‹1: æ¼”ç»æ¨ç†")
    reasoning_engine.symbolic_engine.add_rule("æ‰€æœ‰äººéƒ½ä¼šæ­»", "è‹æ ¼æ‹‰åº•ä¼šæ­»")
    reasoning_engine.symbolic_engine.add_fact("æ‰€æœ‰äººéƒ½ä¼šæ­»")
    
    conclusions = reasoning_engine.multi_modal_reasoning(
        query="è‹æ ¼æ‹‰åº•ä¼šæ­»",
        premises=["è‹æ ¼æ‹‰åº•æ˜¯äºº", "æ‰€æœ‰äººéƒ½ä¼šæ­»"],
        reasoning_types=[ReasoningType.DEDUCTIVE]
    )
    
    for conclusion in conclusions:
        print(reasoning_engine.explain_reasoning(conclusion))
        
    # ç¤ºä¾‹2: å› æœæ¨ç†
    print("\nğŸ”— ç¤ºä¾‹2: å› æœæ¨ç†")
    reasoning_engine.causal_engine.add_causal_relation("ä¸‹é›¨", "åœ°é¢æ¹¿æ¶¦", 0.9)
    reasoning_engine.causal_engine.add_causal_relation("åœ°é¢æ¹¿æ¶¦", "è·¯æ»‘", 0.8)
    reasoning_engine.causal_engine.add_causal_relation("è·¯æ»‘", "äº¤é€šäº‹æ•…", 0.6)
    
    conclusions = reasoning_engine.multi_modal_reasoning(
        query="äº¤é€šäº‹æ•…",
        premises=["ä¸‹é›¨", "åœ°é¢æ¹¿æ¶¦", "è·¯æ»‘"],
        reasoning_types=[ReasoningType.CAUSAL]
    )
    
    for conclusion in conclusions:
        print(reasoning_engine.explain_reasoning(conclusion))
        
    # ç¤ºä¾‹3: ç±»æ¯”æ¨ç†
    print("\nğŸ”„ ç¤ºä¾‹3: ç±»æ¯”æ¨ç†")
    reasoning_engine.analogical_engine.add_analogy(
        source={"ç»“æ„": "åŸå­æ ¸", "å›´ç»•": "ç”µå­"},
        target={"ç»“æ„": "å¤ªé˜³", "å›´ç»•": "è¡Œæ˜Ÿ"},
        mapping={"åŸå­æ ¸": "å¤ªé˜³", "ç”µå­": "è¡Œæ˜Ÿ"}
    )
    
    conclusions = reasoning_engine.multi_modal_reasoning(
        query="è¡Œæ˜Ÿç»•å¤ªé˜³è¿è¡Œ",
        premises=["ç”µå­ç»•åŸå­æ ¸è¿è¡Œ", "åŸå­æ¨¡å‹"],
        reasoning_types=[ReasoningType.ANALOGICAL]
    )
    
    for conclusion in conclusions:
        print(reasoning_engine.explain_reasoning(conclusion))
        
    # æ€§èƒ½æŠ¥å‘Š
    print("\nğŸ“Š æ€§èƒ½æŠ¥å‘Š")
    report = reasoning_engine.get_performance_report()
    for key, value in report.items():
        print(f"{key}: {value}")
        
    print("\nâœ… ç¬¬1è½®æå‡å®Œæˆï¼é«˜çº§æ¨ç†å¼•æ“2.0å·²æˆåŠŸéƒ¨ç½²")

if __name__ == "__main__":
    asyncio.run(demonstrate_hybrid_reasoning())