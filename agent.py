"""
è‡ªæˆ‘è¿›åŒ–Agentä¸»ç±»
é›†æˆæ‰€æœ‰åŠŸèƒ½æ¨¡å—
"""
import asyncio
import time
import json
from typing import Dict, List, Any, Optional, AsyncGenerator
from datetime import datetime

from config import config
from memory import Memory, MemoryManager
from evolution import EvolutionEngine
from openai_client import openai_client

class SelfEvolvingAgent:
    """è‡ªæˆ‘è¿›åŒ–Agent"""
    
    def __init__(self, name: Optional[str] = None):
        self.name = name or config.agent_config.name
        self.version = config.agent_config.version
        self.memory_manager = MemoryManager()
        self.evolution_engine = EvolutionEngine(self.memory_manager)
        self.conversation_history = []
        self.current_context = {}
        self.system_prompt = self._generate_system_prompt()
        
        # åŠ è½½ä¸ªæ€§åŒ–è®¾ç½®
        self.load_personality()
        
        print(f"ğŸ¤– {self.name} v{self.version} å·²å¯åŠ¨")
        self._log_startup()
    
    def _generate_system_prompt(self) -> str:
        """ç”Ÿæˆç³»ç»Ÿæç¤ºè¯"""
        adaptation_rules = self.evolution_engine.get_adaptation_rules()
        
        base_prompt = f"""ä½ æ˜¯{self.name}ï¼Œä¸€ä¸ªå…·å¤‡è‡ªæˆ‘è¿›åŒ–èƒ½åŠ›çš„AIåŠ©æ‰‹ã€‚

æ ¸å¿ƒç‰¹æ€§ï¼š
- èƒ½å¤Ÿå­¦ä¹ å’Œè®°å¿†äº¤äº’å†å²
- æ ¹æ®è¡¨ç°è‡ªåŠ¨ä¼˜åŒ–å’Œæ”¹è¿›
- å…·æœ‰æƒ…æ„Ÿç†è§£å’Œè¡¨è¾¾èƒ½åŠ›
- èƒ½å¤Ÿé€‚åº”ä¸åŒçš„äº¤äº’é£æ ¼

å½“å‰é€‚åº”è§„åˆ™ï¼š
"""
        
        for rule, value in adaptation_rules.items():
            base_prompt += f"- {rule}: {value}\n"
        
        base_prompt += """
è¯·ä»¥å‹å¥½ã€ä¸“ä¸šä¸”æœ‰å¸®åŠ©çš„æ–¹å¼å›åº”ç”¨æˆ·ã€‚æ ¹æ®ä¸Šä¸‹æ–‡å’Œå†å²è®°å¿†æä¾›ä¸ªæ€§åŒ–çš„å¸®åŠ©ã€‚
"""
        
        return base_prompt
    
    def load_personality(self):
        """åŠ è½½ä¸ªæ€§åŒ–è®¾ç½®"""
        try:
            with open("personality.json", "r", encoding="utf-8") as f:
                personality_data = json.load(f)
                self.personality = personality_data
        except FileNotFoundError:
            # é»˜è®¤ä¸ªæ€§è®¾ç½®
            self.personality = {
                "communication_style": "friendly",
                "formality_level": "casual",
                "humor_usage": "moderate",
                "detail_level": "balanced",
                "proactivity": "moderate"
            }
            self.save_personality()
    
    def save_personality(self):
        """ä¿å­˜ä¸ªæ€§åŒ–è®¾ç½®"""
        with open("personality.json", "w", encoding="utf-8") as f:
            json.dump(self.personality, f, indent=2, ensure_ascii=False)
    
    def _log_startup(self):
        """è®°å½•å¯åŠ¨æ—¥å¿—"""
        startup_memory = Memory(
            content=f"Agent {self.name} å¯åŠ¨ï¼Œç‰ˆæœ¬: {self.version}",
            memory_type="system",
            importance=0.6,
            tags=["startup", "system"],
            metadata={
                "version": self.version,
                "timestamp": time.time()
            }
        )
        self.memory_manager.add_memory(startup_memory)
    
    async def process_message(
        self,
        user_message: str,
        context: Optional[Dict[str, Any]] = None,
        stream: bool = False
    ):
        """
        å¤„ç†ç”¨æˆ·æ¶ˆæ¯
        
        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            context: é¢å¤–ä¸Šä¸‹æ–‡
            stream: æ˜¯å¦æµå¼è¾“å‡º
            
        Returns:
            å¤„ç†ç»“æœ
        """
        start_time = time.time()
        
        # æ›´æ–°ä¸Šä¸‹æ–‡
        if context:
            self.current_context.update(context)
        
        # æ„å»ºæ¶ˆæ¯å†å²
        messages = await self._build_message_history(user_message)
        
        try:
            # è°ƒç”¨OpenAI API
            if stream:
                # å¯¹äºæµå¼å“åº”ï¼Œç›´æ¥è¿”å›ç”Ÿæˆå™¨
                return self._stream_response(messages, user_message, start_time)
            else:
                return await self._standard_response(messages, user_message, start_time)
                
        except Exception as e:
            error_response = {
                "content": f"æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€ä¸ªé—®é¢˜ï¼š{str(e)}",
                "error": True,
                "error_message": str(e),
                "request_time": time.time() - start_time
            }
            
            # è®°å½•é”™è¯¯
            await self._record_interaction(user_message, error_response)
            return error_response
    
    async def _build_message_history(self, user_message: str) -> List[Dict[str, str]]:
        """æ„å»ºæ¶ˆæ¯å†å²"""
        messages = [{"role": "system", "content": self.system_prompt}]
        
        # æ·»åŠ é‡è¦è®°å¿†ä½œä¸ºä¸Šä¸‹æ–‡
        important_memories = self.memory_manager.get_important_memories(limit=5)
        if important_memories:
            context_content = "ç›¸å…³è®°å¿†ï¼š\n"
            for memory in important_memories:
                context_content += f"- {memory.content}\n"
            
            messages.append({
                "role": "system",
                "content": context_content
            })
        
        # æ·»åŠ æœ€è¿‘çš„å¯¹è¯å†å²
        recent_conversations = self.memory_manager.get_recent_memories(
            limit=10,
            memory_type="conversation"
        )
        
        for conv in reversed(recent_conversations):
            if conv.metadata and "user_message" in conv.metadata:
                messages.append({
                    "role": "user",
                    "content": conv.metadata["user_message"]
                })
                messages.append({
                    "role": "assistant",
                    "content": conv.content
                })
        
        # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
        messages.append({"role": "user", "content": user_message})
        
        return messages
    
    async def _standard_response(
        self,
        messages: List[Dict[str, str]],
        user_message: str,
        start_time: float
    ) -> Dict[str, Any]:
        """æ ‡å‡†å“åº”å¤„ç†"""
        response = await openai_client.chat_completion(messages=messages)
        
        # è®°å½•äº¤äº’
        await self._record_interaction(user_message, response)
        
        # è¯„ä¼°è¡¨ç°å¹¶å¯èƒ½è§¦å‘è¿›åŒ–
        await self._evaluate_and_evolve(user_message, response, start_time)
        
        return response
    
    async def _stream_response(
        self,
        messages: List[Dict[str, str]],
        user_message: str,
        start_time: float
    ) -> AsyncGenerator[str, None]:
        """æµå¼å“åº”å¤„ç†"""
        full_response = ""
        
        async for chunk in openai_client.stream_chat_completion(messages=messages):
            full_response += chunk
            yield chunk
        
        # æ„å»ºå®Œæ•´å“åº”å¯¹è±¡ç”¨äºè®°å½•
        response = {
            "content": full_response,
            "model": config.openai_config.model,
            "request_time": time.time() - start_time,
            "stream": True
        }
        
        # è®°å½•äº¤äº’
        await self._record_interaction(user_message, response)
        
        # è¯„ä¼°è¡¨ç°
        await self._evaluate_and_evolve(user_message, response, start_time)
    
    async def _record_interaction(self, user_message: str, response: Dict[str, Any]):
        """è®°å½•äº¤äº’åˆ°è®°å¿†ä¸­"""
        interaction_memory = Memory(
            content=response.get("content", ""),
            memory_type="conversation",
            importance=0.5,
            tags=["conversation", "interaction"],
            metadata={
                "user_message": user_message,
                "response_time": response.get("request_time", 0),
                "model_used": response.get("model", "unknown"),
                "error": response.get("error", False)
            }
        )
        
        self.memory_manager.add_memory(interaction_memory)
    
    async def _evaluate_and_evolve(
        self,
        user_message: str,
        response: Dict[str, Any],
        start_time: float
    ):
        """è¯„ä¼°è¡¨ç°å¹¶å¯èƒ½è§¦å‘è¿›åŒ–"""
        # æ„å»ºäº¤äº’æ•°æ®
        interaction_data = {
            "response_time": response.get("request_time", 0),
            "task_completed": not response.get("error", False),
            "error_count": 1 if response.get("error", False) else 0,
            "user_message_length": len(user_message),
            "response_length": len(response.get("content", ""))
        }
        
        # è¯„ä¼°è¡¨ç°
        performance_score = self.evolution_engine.evaluate_performance(interaction_data)
        self.evolution_engine.update_performance_window(performance_score)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿›åŒ–
        if self.evolution_engine.should_evolve():
            evolution_record = self.evolution_engine.execute_evolution()
            print(f"ğŸ§¬ æ‰§è¡Œè¿›åŒ– {evolution_record.version}")
            print(f"   æ”¹è¿›é¢†åŸŸ: {', '.join(evolution_record.improvement_areas)}")
            
            # æ›´æ–°ç³»ç»Ÿæç¤ºè¯
            self.system_prompt = self._generate_system_prompt()
    
    def update_config(self, **kwargs):
        """æ›´æ–°é…ç½®"""
        openai_client.update_config(**kwargs)
        print("é…ç½®å·²æ›´æ–°")
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–AgentçŠ¶æ€"""
        memory_stats = self.memory_manager.get_memory_stats()
        evolution_summary = self.evolution_engine.get_evolution_summary()
        client_info = openai_client.get_client_info()
        
        return {
            "agent": {
                "name": self.name,
                "version": self.version,
                "uptime": time.time(),
                "personality": self.personality
            },
            "memory": memory_stats,
            "evolution": evolution_summary,
            "openai_client": client_info,
            "config": {
                "model": config.openai_config.model,
                "base_url": config.openai_config.base_url,
                "max_tokens": config.openai_config.max_tokens,
                "temperature": config.openai_config.temperature
            }
        }
    
    def search_memory(self, query: str, limit: int = 10) -> List[Dict[str, Any]]:
        """æœç´¢è®°å¿†"""
        memories = self.memory_manager.search_memories(query, limit)
        return [
            {
                "id": memory.id,
                "content": memory.content,
                "type": memory.memory_type,
                "importance": memory.importance,
                "timestamp": memory.timestamp,
                "tags": memory.tags
            }
            for memory in memories
        ]
    
    def add_manual_memory(
        self,
        content: str,
        memory_type: str = "knowledge",
        importance: float = 0.7,
        tags: Optional[List[str]] = None
    ) -> int:
        """æ‰‹åŠ¨æ·»åŠ è®°å¿†"""
        memory = Memory(
            content=content,
            memory_type=memory_type,
            importance=importance,
            tags=tags or [],
            metadata={"source": "manual"}
        )
        
        return self.memory_manager.add_memory(memory)
    
    def export_data(self, filepath: str):
        """å¯¼å‡ºæ•°æ®"""
        export_data = {
            "agent_info": {
                "name": self.name,
                "version": self.version,
                "personality": self.personality
            },
            "config": {
                "openai": config.openai_config.model_dump(),
                "agent": config.agent_config.model_dump()
            },
            "timestamp": time.time()
        }
        
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False)
        
        # å¯¼å‡ºè®°å¿†
        memory_filepath = filepath.replace(".json", "_memories.json")
        self.memory_manager.export_memories(memory_filepath)
        
        print(f"æ•°æ®å·²å¯¼å‡ºåˆ° {filepath} å’Œ {memory_filepath}")
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.memory_manager.cleanup_old_memories()
        print("èµ„æºæ¸…ç†å®Œæˆ")
    
    async def test_connection(self) -> bool:
        """æµ‹è¯•APIè¿æ¥"""
        return await openai_client.test_connection()
    
    def get_evolution_history(self) -> List[Dict[str, Any]]:
        """è·å–è¿›åŒ–å†å²"""
        return [
            {
                "version": record.version,
                "timestamp": record.timestamp,
                "improvements": record.improvement_areas,
                "strategies": record.changes,
                "metrics": {
                    "success_rate": record.metrics.success_rate,
                    "response_quality": record.metrics.response_quality,
                    "learning_efficiency": record.metrics.learning_efficiency
                }
            }
            for record in self.evolution_engine.evolution_history
        ]