"""
è‡ªä¸»è¿›åŒ–Agentç³»ç»Ÿ
åŸºäºæœ€æ–°ç ”ç©¶å®ç°çš„è‡ªæˆ‘è¿›åŒ–ã€è‡ªæˆ‘ä¼˜åŒ–AI Agentæ¡†æ¶

ä¸»è¦ç‰¹æ€§ï¼š
1. ReActæ¶æ„ - æ¨ç†ä¸è¡ŒåŠ¨å¾ªç¯
2. å¤šAgentåä½œ - è§’è‰²ä¸“ä¸šåŒ–
3. è‡ªæˆ‘è¯„ä¼°ä¸æ”¹è¿›
4. è®­ç»ƒæ— å…³è¯„ä¼°ç³»ç»Ÿ
5. è®°å¿†ä¸å­¦ä¹ æœºåˆ¶
"""

import asyncio
import json
import logging
import uuid
import time
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any, Callable, Tuple
from enum import Enum
import numpy as np
from datetime import datetime
import pickle
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed


# æ—¥å¿—é…ç½®
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class AgentRole(Enum):
    """Agentè§’è‰²å®šä¹‰"""
    RESEARCHER = "researcher"          # ç ”ç©¶è€… - ä¿¡æ¯æ”¶é›†ä¸åˆ†æ
    EXECUTOR = "executor"             # æ‰§è¡Œè€… - ä»»åŠ¡æ‰§è¡Œ
    CRITIC = "critic"                 # è¯„åˆ¤è€… - æ€§èƒ½è¯„ä¼°
    COORDINATOR = "coordinator"       # åè°ƒè€… - ä»»åŠ¡åˆ†é…ä¸åè°ƒ
    OPTIMIZER = "optimizer"           # ä¼˜åŒ–è€… - è‡ªæˆ‘æ”¹è¿›
    MEMORY_MANAGER = "memory_manager" # è®°å¿†ç®¡ç†è€… - çŸ¥è¯†å­˜å‚¨ä¸æ£€ç´¢


class ActionType(Enum):
    """è¡ŒåŠ¨ç±»å‹"""
    THINK = "think"
    OBSERVE = "observe"
    EXECUTE = "execute"
    COMMUNICATE = "communicate"
    LEARN = "learn"
    SELF_MODIFY = "self_modify"


@dataclass
class AgentAction:
    """Agentè¡ŒåŠ¨æ•°æ®ç»“æ„"""
    action_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    agent_id: str = ""
    action_type: ActionType = ActionType.THINK
    content: Any = None
    timestamp: datetime = field(default_factory=datetime.now)
    success: bool = True
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class AgentMemory:
    """Agentè®°å¿†æ•°æ®ç»“æ„"""
    memory_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    content: Any = None
    memory_type: str = "general"
    importance: float = 0.5
    timestamp: datetime = field(default_factory=datetime.now)
    access_count: int = 0
    success_rate: float = 0.0


@dataclass
class EvaluationMetrics:
    """è¯„ä¼°æŒ‡æ ‡"""
    trainability: float = 0.0      # å¯è®­ç»ƒæ€§ (ST)
    generalization: float = 0.0    # æ³›åŒ–èƒ½åŠ› (SG)
    expressiveness: float = 0.0    # è¡¨è¾¾èƒ½åŠ› (SE)
    composite_score: float = 0.0   # ç»¼åˆå¾—åˆ†
    execution_time: float = 0.0    # æ‰§è¡Œæ—¶é—´
    resource_usage: Dict[str, float] = field(default_factory=dict)


class CommunicationProtocol:
    """Agenté—´é€šä¿¡åè®®"""
    
    def __init__(self):
        self.message_queue: Dict[str, List[Dict]] = {}
        self.subscribers: Dict[str, List[str]] = {}
        self.lock = threading.Lock()
    
    def subscribe(self, topic: str, agent_id: str):
        """è®¢é˜…ä¸»é¢˜"""
        with self.lock:
            if topic not in self.subscribers:
                self.subscribers[topic] = []
            if agent_id not in self.subscribers[topic]:
                self.subscribers[topic].append(agent_id)
    
    def publish(self, topic: str, message: Dict, sender_id: str):
        """å‘å¸ƒæ¶ˆæ¯"""
        with self.lock:
            if topic in self.subscribers:
                for agent_id in self.subscribers[topic]:
                    if agent_id != sender_id:  # ä¸å‘é€ç»™è‡ªå·±
                        if agent_id not in self.message_queue:
                            self.message_queue[agent_id] = []
                        self.message_queue[agent_id].append({
                            'topic': topic,
                            'message': message,
                            'sender': sender_id,
                            'timestamp': datetime.now()
                        })
    
    def get_messages(self, agent_id: str) -> List[Dict]:
        """è·å–æ¶ˆæ¯"""
        with self.lock:
            messages = self.message_queue.get(agent_id, [])
            self.message_queue[agent_id] = []  # æ¸…ç©ºå·²è¯»æ¶ˆæ¯
            return messages


class TrainingFreeEvaluator:
    """è®­ç»ƒæ— å…³è¯„ä¼°å™¨ - åŸºäºæœ€æ–°ç ”ç©¶çš„å¤šæŒ‡æ ‡è¯„ä¼°"""
    
    @staticmethod
    def calculate_trainability(gradients: List[float], dataset_size: int = 1000, 
                             lipschitz_constant: float = 1.0, batch_size: int = 32) -> float:
        """è®¡ç®—å¯è®­ç»ƒæ€§æŒ‡æ ‡ (ST)"""
        if not gradients:
            return 0.0
        
        gradient_norm_squared = sum(g**2 for g in gradients)
        st = (dataset_size / (lipschitz_constant * batch_size)) * gradient_norm_squared
        return min(st / 1000.0, 1.0)  # å½’ä¸€åŒ–åˆ°[0,1]
    
    @staticmethod
    def calculate_generalization(original_output: List[float], 
                               noisy_output: List[float]) -> float:
        """è®¡ç®—æ³›åŒ–èƒ½åŠ›æŒ‡æ ‡ (SG)"""
        if len(original_output) != len(noisy_output):
            return 0.0
        
        differences = [(o - n)**2 for o, n in zip(original_output, noisy_output)]
        sg = sum(differences)
        return max(0.0, 1.0 - sg / len(differences))  # è½¬æ¢ä¸º[0,1]èŒƒå›´ï¼Œå€¼è¶Šé«˜è¶Šå¥½
    
    @staticmethod
    def calculate_expressiveness(complexity_score: float, 
                               variance_stats: List[float]) -> float:
        """è®¡ç®—è¡¨è¾¾èƒ½åŠ›æŒ‡æ ‡ (SE)"""
        if not variance_stats:
            return complexity_score
        
        variance_term = sum(np.log(max(v, 1e-8)) for v in variance_stats) / len(variance_stats)
        se = complexity_score + variance_term
        return max(0.0, min(se / 10.0, 1.0))  # å½’ä¸€åŒ–
    
    @staticmethod
    def calculate_composite_score(st: float, sg: float, se: float, epsilon: float = 1e-8) -> float:
        """è®¡ç®—ç»¼åˆå¾—åˆ† - ä½¿ç”¨å¯¹æ•°-æŒ‡æ•°å˜æ¢"""
        metrics = [st, sg, se]
        ranks = [sorted(metrics, reverse=True).index(m) + 1 for m in metrics]
        
        composite = sum(np.exp(-np.log(rank + epsilon)) for rank in ranks)
        return composite / 3.0  # å½’ä¸€åŒ–


class BaseAgent(ABC):
    """åŸºç¡€AgentæŠ½è±¡ç±»"""
    
    def __init__(self, agent_id: str, role: AgentRole, 
                 communication: CommunicationProtocol):
        self.agent_id = agent_id
        self.role = role
        self.communication = communication
        self.memory: List[AgentMemory] = []
        self.action_history: List[AgentAction] = []
        self.performance_metrics: List[EvaluationMetrics] = []
        self.is_active = True
        self.learning_rate = 0.1
        self.temperature = 0.7  # åˆ›é€ æ€§å‚æ•°
        
        # è®¢é˜…ç›¸å…³ä¸»é¢˜
        self.communication.subscribe("global_broadcast", self.agent_id)
        self.communication.subscribe(f"agent_{self.agent_id}", self.agent_id)
        self.communication.subscribe(f"role_{self.role.value}", self.agent_id)
    
    @abstractmethod
    async def think(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """æ€è€ƒ - ç”Ÿæˆè¡ŒåŠ¨è®¡åˆ’"""
        pass
    
    @abstractmethod
    async def act(self, plan: Dict[str, Any]) -> AgentAction:
        """è¡ŒåŠ¨ - æ‰§è¡Œè®¡åˆ’"""
        pass
    
    @abstractmethod
    async def observe(self, action_result: AgentAction) -> Dict[str, Any]:
        """è§‚å¯Ÿ - åˆ†æè¡ŒåŠ¨ç»“æœ"""
        pass
    
    async def react_cycle(self, initial_context: Dict[str, Any]) -> List[AgentAction]:
        """ReActå¾ªç¯ - æ€è€ƒ-è¡ŒåŠ¨-è§‚å¯Ÿ"""
        actions = []
        context = initial_context.copy()
        max_iterations = 10
        
        for iteration in range(max_iterations):
            try:
                # æ£€æŸ¥æ¶ˆæ¯
                messages = self.communication.get_messages(self.agent_id)
                if messages:
                    context['messages'] = messages
                
                # æ€è€ƒ
                plan = await self.think(context)
                if not plan or plan.get('stop', False):
                    break
                
                # è¡ŒåŠ¨
                action = await self.act(plan)
                actions.append(action)
                
                # è§‚å¯Ÿ
                observation = await self.observe(action)
                context.update(observation)
                
                # å­¦ä¹ 
                await self.learn_from_action(action, observation)
                
                # å¦‚æœä»»åŠ¡å®Œæˆåˆ™é€€å‡º
                if observation.get('task_completed', False):
                    break
                    
            except Exception as e:
                logger.error(f"Agent {self.agent_id} ReAct cycle error: {e}")
                break
        
        return actions
    
    async def learn_from_action(self, action: AgentAction, observation: Dict[str, Any]):
        """ä»è¡ŒåŠ¨ä¸­å­¦ä¹ """
        # è¯„ä¼°è¡ŒåŠ¨æ•ˆæœ
        success_score = observation.get('success_score', 0.5)
        
        # æ›´æ–°è®°å¿†
        memory = AgentMemory(
            content={
                'action': action,
                'observation': observation,
                'success_score': success_score
            },
            memory_type='action_result',
            importance=success_score,
            success_rate=success_score
        )
        self.memory.append(memory)
        
        # ä¿æŒè®°å¿†æ•°é‡é™åˆ¶
        if len(self.memory) > 1000:
            # ç§»é™¤é‡è¦æ€§æœ€ä½çš„è®°å¿†
            self.memory.sort(key=lambda m: m.importance)
            self.memory = self.memory[100:]
    
    def get_relevant_memories(self, context: Dict[str, Any], limit: int = 5) -> List[AgentMemory]:
        """è·å–ç›¸å…³è®°å¿†"""
        # ç®€å•çš„ç›¸å…³æ€§åŒ¹é… - å®é™…å®ç°å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„ç›¸ä¼¼åº¦è®¡ç®—
        relevant_memories = []
        
        for memory in self.memory:
            if memory.memory_type == 'action_result':
                memory.access_count += 1
                relevant_memories.append(memory)
        
        # æŒ‰é‡è¦æ€§å’ŒæˆåŠŸç‡æ’åº
        relevant_memories.sort(
            key=lambda m: m.importance * m.success_rate, 
            reverse=True
        )
        
        return relevant_memories[:limit]
    
    async def self_evaluate(self) -> EvaluationMetrics:
        """è‡ªæˆ‘è¯„ä¼°"""
        if not self.action_history:
            return EvaluationMetrics()
        
        # è®¡ç®—å„é¡¹æŒ‡æ ‡
        recent_actions = self.action_history[-10:]  # æœ€è¿‘10ä¸ªè¡ŒåŠ¨
        
        # æ¨¡æ‹Ÿæ¢¯åº¦è®¡ç®—
        gradients = [np.random.normal(0, 1) for _ in range(len(recent_actions))]
        
        # æ¨¡æ‹Ÿè¾“å‡ºå¯¹æ¯”
        original_output = [a.metadata.get('output_score', 0.5) for a in recent_actions]
        noisy_output = [o + np.random.normal(0, 0.1) for o in original_output]
        
        # è®¡ç®—æŒ‡æ ‡
        evaluator = TrainingFreeEvaluator()
        st = evaluator.calculate_trainability(gradients)
        sg = evaluator.calculate_generalization(original_output, noisy_output)
        se = evaluator.calculate_expressiveness(
            np.mean(original_output), 
            [abs(g) for g in gradients]
        )
        composite = evaluator.calculate_composite_score(st, sg, se)
        
        metrics = EvaluationMetrics(
            trainability=st,
            generalization=sg,
            expressiveness=se,
            composite_score=composite,
            execution_time=sum(a.metadata.get('execution_time', 0) for a in recent_actions),
            resource_usage={'memory': len(self.memory), 'actions': len(self.action_history)}
        )
        
        self.performance_metrics.append(metrics)
        return metrics
    
    async def self_improve(self, metrics: EvaluationMetrics):
        """è‡ªæˆ‘æ”¹è¿›"""
        # åŸºäºè¯„ä¼°ç»“æœè°ƒæ•´å‚æ•°
        if metrics.composite_score < 0.3:
            # æ€§èƒ½è¾ƒå·®ï¼Œå¢åŠ æ¢ç´¢æ€§
            self.temperature = min(1.0, self.temperature + 0.1)
            self.learning_rate = min(0.5, self.learning_rate + 0.05)
        elif metrics.composite_score > 0.7:
            # æ€§èƒ½è‰¯å¥½ï¼Œå‡å°‘æ¢ç´¢ï¼Œå¢åŠ åˆ©ç”¨
            self.temperature = max(0.1, self.temperature - 0.05)
            self.learning_rate = max(0.01, self.learning_rate - 0.01)
        
        # è®°å½•æ”¹è¿›è¡ŒåŠ¨
        improvement_action = AgentAction(
            agent_id=self.agent_id,
            action_type=ActionType.SELF_MODIFY,
            content={
                'old_temperature': self.temperature,
                'old_learning_rate': self.learning_rate,
                'metrics': metrics
            },
            metadata={'improvement_type': 'parameter_adjustment'}
        )
        self.action_history.append(improvement_action)
        
        logger.info(f"Agent {self.agent_id} self-improved: temp={self.temperature:.2f}, lr={self.learning_rate:.3f}")


class ResearcherAgent(BaseAgent):
    """ç ”ç©¶è€…Agent - è´Ÿè´£ä¿¡æ¯æ”¶é›†ä¸åˆ†æ"""
    
    def __init__(self, agent_id: str, communication: CommunicationProtocol):
        super().__init__(agent_id, AgentRole.RESEARCHER, communication)
        self.knowledge_base = {}
    
    async def think(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """ç ”ç©¶è€…æ€è€ƒè¿‡ç¨‹"""
        goal = context.get('goal', 'general_research')
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸å…³çŸ¥è¯†
        relevant_memories = self.get_relevant_memories(context)
        
        plan = {
            'action_type': 'research',
            'target': goal,
            'methods': ['web_search', 'knowledge_lookup', 'analysis'],
            'use_memories': len(relevant_memories) > 0
        }
        
        return plan
    
    async def act(self, plan: Dict[str, Any]) -> AgentAction:
        """æ‰§è¡Œç ”ç©¶ä»»åŠ¡"""
        start_time = time.time()
        
        # æ¨¡æ‹Ÿç ”ç©¶è¿‡ç¨‹
        research_result = {
            'findings': f"Research findings for {plan['target']}",
            'confidence': np.random.uniform(0.6, 0.9),
            'sources': ['source1', 'source2', 'source3']
        }
        
        # å¹¿æ’­ç ”ç©¶ç»“æœ
        self.communication.publish(
            'research_results',
            research_result,
            self.agent_id
        )
        
        action = AgentAction(
            agent_id=self.agent_id,
            action_type=ActionType.EXECUTE,
            content=research_result,
            metadata={
                'execution_time': time.time() - start_time,
                'output_score': research_result['confidence']
            }
        )
        
        self.action_history.append(action)
        return action
    
    async def observe(self, action_result: AgentAction) -> Dict[str, Any]:
        """è§‚å¯Ÿç ”ç©¶ç»“æœ"""
        success_score = action_result.content.get('confidence', 0.5)
        
        observation = {
            'success_score': success_score,
            'task_completed': success_score > 0.8,
            'next_action': 'refine_research' if success_score < 0.7 else 'complete'
        }
        
        return observation


class ExecutorAgent(BaseAgent):
    """æ‰§è¡Œè€…Agent - è´Ÿè´£ä»»åŠ¡æ‰§è¡Œ"""
    
    def __init__(self, agent_id: str, communication: CommunicationProtocol):
        super().__init__(agent_id, AgentRole.EXECUTOR, communication)
        self.execution_capabilities = ['code_generation', 'task_automation', 'system_interaction']
    
    async def think(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œè€…æ€è€ƒè¿‡ç¨‹"""
        task = context.get('task', 'general_execution')
        
        plan = {
            'action_type': 'execute',
            'task': task,
            'approach': 'step_by_step',
            'estimated_difficulty': np.random.uniform(0.3, 0.8)
        }
        
        return plan
    
    async def act(self, plan: Dict[str, Any]) -> AgentAction:
        """æ‰§è¡Œä»»åŠ¡"""
        start_time = time.time()
        
        # æ¨¡æ‹Ÿä»»åŠ¡æ‰§è¡Œ
        difficulty = plan['estimated_difficulty']
        success_probability = max(0.1, 1.0 - difficulty)
        
        execution_result = {
            'task': plan['task'],
            'success': np.random.random() < success_probability,
            'output': f"Execution result for {plan['task']}",
            'efficiency': np.random.uniform(0.5, 1.0)
        }
        
        action = AgentAction(
            agent_id=self.agent_id,
            action_type=ActionType.EXECUTE,
            content=execution_result,
            success=execution_result['success'],
            metadata={
                'execution_time': time.time() - start_time,
                'output_score': execution_result['efficiency']
            }
        )
        
        self.action_history.append(action)
        return action
    
    async def observe(self, action_result: AgentAction) -> Dict[str, Any]:
        """è§‚å¯Ÿæ‰§è¡Œç»“æœ"""
        success_score = 1.0 if action_result.success else 0.2
        
        observation = {
            'success_score': success_score,
            'task_completed': action_result.success,
            'next_action': 'complete' if action_result.success else 'retry'
        }
        
        return observation


class CriticAgent(BaseAgent):
    """è¯„åˆ¤è€…Agent - è´Ÿè´£æ€§èƒ½è¯„ä¼°å’Œè´¨é‡æ§åˆ¶"""
    
    def __init__(self, agent_id: str, communication: CommunicationProtocol):
        super().__init__(agent_id, AgentRole.CRITIC, communication)
        self.evaluation_criteria = ['accuracy', 'efficiency', 'completeness', 'creativity']
    
    async def think(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """è¯„åˆ¤è€…æ€è€ƒè¿‡ç¨‹"""
        target_actions = context.get('actions_to_evaluate', [])
        
        plan = {
            'action_type': 'evaluate',
            'targets': target_actions,
            'criteria': self.evaluation_criteria,
            'evaluation_depth': 'comprehensive'
        }
        
        return plan
    
    async def act(self, plan: Dict[str, Any]) -> AgentAction:
        """æ‰§è¡Œè¯„ä¼°"""
        start_time = time.time()
        
        evaluations = []
        for target in plan['targets']:
            evaluation = {
                'target': target,
                'scores': {criterion: np.random.uniform(0.3, 1.0) 
                          for criterion in self.evaluation_criteria},
                'overall_score': np.random.uniform(0.4, 0.9),
                'recommendations': [f"Improve {criterion}" 
                                  for criterion in self.evaluation_criteria[:2]]
            }
            evaluations.append(evaluation)
        
        # å¹¿æ’­è¯„ä¼°ç»“æœ
        self.communication.publish(
            'evaluation_results',
            {'evaluations': evaluations},
            self.agent_id
        )
        
        action = AgentAction(
            agent_id=self.agent_id,
            action_type=ActionType.EXECUTE,
            content={'evaluations': evaluations},
            metadata={
                'execution_time': time.time() - start_time,
                'output_score': np.mean([e['overall_score'] for e in evaluations])
            }
        )
        
        self.action_history.append(action)
        return action
    
    async def observe(self, action_result: AgentAction) -> Dict[str, Any]:
        """è§‚å¯Ÿè¯„ä¼°ç»“æœ"""
        evaluations = action_result.content.get('evaluations', [])
        average_score = np.mean([e['overall_score'] for e in evaluations]) if evaluations else 0.5
        
        observation = {
            'success_score': average_score,
            'task_completed': True,
            'insights': f"Evaluated {len(evaluations)} items with average score {average_score:.2f}"
        }
        
        return observation


class CoordinatorAgent(BaseAgent):
    """åè°ƒè€…Agent - è´Ÿè´£ä»»åŠ¡åˆ†é…å’Œåè°ƒ"""
    
    def __init__(self, agent_id: str, communication: CommunicationProtocol):
        super().__init__(agent_id, AgentRole.COORDINATOR, communication)
        self.managed_agents = []
        self.task_queue = []
    
    def register_agent(self, agent: BaseAgent):
        """æ³¨å†Œè¢«ç®¡ç†çš„Agent"""
        self.managed_agents.append(agent)
    
    async def think(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """åè°ƒè€…æ€è€ƒè¿‡ç¨‹"""
        goal = context.get('goal', 'coordinate_tasks')
        available_agents = [agent for agent in self.managed_agents if agent.is_active]
        
        plan = {
            'action_type': 'coordinate',
            'goal': goal,
            'available_agents': len(available_agents),
            'coordination_strategy': 'balanced_allocation'
        }
        
        return plan
    
    async def act(self, plan: Dict[str, Any]) -> AgentAction:
        """æ‰§è¡Œåè°ƒä»»åŠ¡"""
        start_time = time.time()
        
        # åˆ†é…ä»»åŠ¡ç»™ä¸åŒè§’è‰²çš„Agent
        task_assignments = {
            'research_tasks': [agent for agent in self.managed_agents 
                             if agent.role == AgentRole.RESEARCHER],
            'execution_tasks': [agent for agent in self.managed_agents 
                              if agent.role == AgentRole.EXECUTOR],
            'evaluation_tasks': [agent for agent in self.managed_agents 
                               if agent.role == AgentRole.CRITIC]
        }
        
        coordination_result = {
            'assignments': task_assignments,
            'coordination_efficiency': np.random.uniform(0.6, 0.9)
        }
        
        # å¹¿æ’­åè°ƒç»“æœ
        self.communication.publish(
            'coordination_update',
            coordination_result,
            self.agent_id
        )
        
        action = AgentAction(
            agent_id=self.agent_id,
            action_type=ActionType.COMMUNICATE,
            content=coordination_result,
            metadata={
                'execution_time': time.time() - start_time,
                'output_score': coordination_result['coordination_efficiency']
            }
        )
        
        self.action_history.append(action)
        return action
    
    async def observe(self, action_result: AgentAction) -> Dict[str, Any]:
        """è§‚å¯Ÿåè°ƒç»“æœ"""
        efficiency = action_result.content.get('coordination_efficiency', 0.5)
        
        observation = {
            'success_score': efficiency,
            'task_completed': efficiency > 0.7,
            'next_action': 'monitor_progress'
        }
        
        return observation


class AutonomousEvolutionarySystem:
    """è‡ªä¸»è¿›åŒ–ç³»ç»Ÿ - ç®¡ç†å¤šä¸ªAgentçš„åä½œå’Œè¿›åŒ–"""
    
    def __init__(self):
        self.communication = CommunicationProtocol()
        self.agents: Dict[str, BaseAgent] = {}
        self.system_metrics: List[Dict[str, Any]] = []
        self.evolution_cycles = 0
        self.executor = ThreadPoolExecutor(max_workers=10)
    
    def add_agent(self, agent: BaseAgent):
        """æ·»åŠ Agentåˆ°ç³»ç»Ÿ"""
        self.agents[agent.agent_id] = agent
        logger.info(f"Added agent {agent.agent_id} with role {agent.role.value}")
    
    def create_standard_team(self) -> Dict[str, BaseAgent]:
        """åˆ›å»ºæ ‡å‡†å›¢é˜Ÿ"""
        team = {}
        
        # åˆ›å»ºå„ç§è§’è‰²çš„Agent
        researcher = ResearcherAgent("researcher_001", self.communication)
        executor = ExecutorAgent("executor_001", self.communication)
        critic = CriticAgent("critic_001", self.communication)
        coordinator = CoordinatorAgent("coordinator_001", self.communication)
        
        # åè°ƒè€…æ³¨å†Œå…¶ä»–Agent
        coordinator.register_agent(researcher)
        coordinator.register_agent(executor)
        coordinator.register_agent(critic)
        
        team = {
            'researcher': researcher,
            'executor': executor,
            'critic': critic,
            'coordinator': coordinator
        }
        
        # æ·»åŠ åˆ°ç³»ç»Ÿ
        for agent in team.values():
            self.add_agent(agent)
        
        return team
    
    async def run_collaborative_task(self, goal: str, max_cycles: int = 5) -> Dict[str, Any]:
        """è¿è¡Œåä½œä»»åŠ¡"""
        logger.info(f"Starting collaborative task: {goal}")
        
        context = {
            'goal': goal,
            'cycle': 0,
            'system_state': 'active'
        }
        
        all_actions = []
        cycle_results = []
        
        for cycle in range(max_cycles):
            context['cycle'] = cycle
            logger.info(f"Starting cycle {cycle + 1}/{max_cycles}")
            
            # å¹¶è¡Œæ‰§è¡Œå¤šä¸ªAgentçš„ReActå¾ªç¯
            cycle_actions = []
            tasks = []
            
            for agent in self.agents.values():
                if agent.is_active:
                    task = asyncio.create_task(agent.react_cycle(context.copy()))
                    tasks.append((agent.agent_id, task))
            
            # ç­‰å¾…æ‰€æœ‰Agentå®Œæˆ
            for agent_id, task in tasks:
                try:
                    actions = await task
                    cycle_actions.extend(actions)
                    logger.info(f"Agent {agent_id} completed with {len(actions)} actions")
                except Exception as e:
                    logger.error(f"Agent {agent_id} failed: {e}")
            
            all_actions.extend(cycle_actions)
            
            # è¯„ä¼°ç³»ç»Ÿæ€§èƒ½
            system_metrics = await self.evaluate_system_performance()
            cycle_results.append({
                'cycle': cycle,
                'actions_count': len(cycle_actions),
                'metrics': system_metrics
            })
            
            # ç³»ç»Ÿè‡ªæˆ‘è¿›åŒ–
            await self.evolve_system(system_metrics)
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
            if system_metrics.composite_score > 0.8:
                logger.info(f"Goal achieved in cycle {cycle + 1}")
                break
        
        final_result = {
            'goal': goal,
            'total_cycles': len(cycle_results),
            'total_actions': len(all_actions),
            'final_metrics': cycle_results[-1]['metrics'] if cycle_results else None,
            'cycle_results': cycle_results,
            'evolution_cycles': self.evolution_cycles
        }
        
        logger.info(f"Task completed: {len(all_actions)} actions in {len(cycle_results)} cycles")
        return final_result
    
    async def evaluate_system_performance(self) -> EvaluationMetrics:
        """è¯„ä¼°ç³»ç»Ÿæ•´ä½“æ€§èƒ½"""
        agent_metrics = []
        
        # æ”¶é›†æ‰€æœ‰Agentçš„è¯„ä¼°ç»“æœ
        for agent in self.agents.values():
            try:
                metrics = await agent.self_evaluate()
                agent_metrics.append(metrics)
            except Exception as e:
                logger.error(f"Failed to evaluate agent {agent.agent_id}: {e}")
        
        if not agent_metrics:
            return EvaluationMetrics()
        
        # è®¡ç®—ç³»ç»Ÿçº§æŒ‡æ ‡
        system_metrics = EvaluationMetrics(
            trainability=np.mean([m.trainability for m in agent_metrics]),
            generalization=np.mean([m.generalization for m in agent_metrics]),
            expressiveness=np.mean([m.expressiveness for m in agent_metrics]),
            composite_score=np.mean([m.composite_score for m in agent_metrics]),
            execution_time=sum([m.execution_time for m in agent_metrics]),
            resource_usage={
                'total_memory': sum([m.resource_usage.get('memory', 0) for m in agent_metrics]),
                'total_actions': sum([m.resource_usage.get('actions', 0) for m in agent_metrics])
            }
        )
        
        self.system_metrics.append({
            'timestamp': datetime.now(),
            'metrics': system_metrics,
            'agent_count': len(self.agents)
        })
        
        return system_metrics
    
    async def evolve_system(self, metrics: EvaluationMetrics):
        """ç³»ç»Ÿè¿›åŒ–"""
        self.evolution_cycles += 1
        
        # åŸºäºæ€§èƒ½æŒ‡æ ‡è¿›åŒ–ç³»ç»Ÿ
        if metrics.composite_score < 0.4:
            # æ€§èƒ½è¾ƒå·®ï¼Œå°è¯•æ·»åŠ æ–°Agentæˆ–è°ƒæ•´ç°æœ‰Agent
            await self.adapt_system_low_performance()
        elif metrics.composite_score > 0.8:
            # æ€§èƒ½è‰¯å¥½ï¼Œä¼˜åŒ–æ•ˆç‡
            await self.optimize_system_high_performance()
        
        # è®©æ‰€æœ‰Agentè¿›è¡Œè‡ªæˆ‘æ”¹è¿›
        for agent in self.agents.values():
            try:
                await agent.self_improve(metrics)
            except Exception as e:
                logger.error(f"Failed to improve agent {agent.agent_id}: {e}")
        
        logger.info(f"System evolution cycle {self.evolution_cycles} completed")
    
    async def adapt_system_low_performance(self):
        """ä½æ€§èƒ½æ—¶çš„ç³»ç»Ÿé€‚åº”"""
        # å¢åŠ æ¢ç´¢æ€§ï¼Œå¯èƒ½æ·»åŠ æ–°çš„Agentç±»å‹
        for agent in self.agents.values():
            agent.temperature = min(1.0, agent.temperature + 0.1)
        
        logger.info("Adapted system for low performance - increased exploration")
    
    async def optimize_system_high_performance(self):
        """é«˜æ€§èƒ½æ—¶çš„ç³»ç»Ÿä¼˜åŒ–"""
        # å‡å°‘æ¢ç´¢ï¼Œå¢åŠ åˆ©ç”¨
        for agent in self.agents.values():
            agent.temperature = max(0.1, agent.temperature - 0.05)
        
        logger.info("Optimized system for high performance - reduced exploration")
    
    def save_system_state(self, filepath: str):
        """ä¿å­˜ç³»ç»ŸçŠ¶æ€"""
        state = {
            'agents': {aid: {
                'role': agent.role.value,
                'memory': agent.memory,
                'action_history': agent.action_history,
                'performance_metrics': agent.performance_metrics,
                'learning_rate': agent.learning_rate,
                'temperature': agent.temperature
            } for aid, agent in self.agents.items()},
            'system_metrics': self.system_metrics,
            'evolution_cycles': self.evolution_cycles
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(state, f)
        
        logger.info(f"System state saved to {filepath}")
    
    def load_system_state(self, filepath: str):
        """åŠ è½½ç³»ç»ŸçŠ¶æ€"""
        try:
            with open(filepath, 'rb') as f:
                state = pickle.load(f)
            
            self.system_metrics = state['system_metrics']
            self.evolution_cycles = state['evolution_cycles']
            
            # é‡å»ºAgent
            for aid, agent_data in state['agents'].items():
                if aid in self.agents:
                    agent = self.agents[aid]
                    agent.memory = agent_data['memory']
                    agent.action_history = agent_data['action_history']
                    agent.performance_metrics = agent_data['performance_metrics']
                    agent.learning_rate = agent_data['learning_rate']
                    agent.temperature = agent_data['temperature']
            
            logger.info(f"System state loaded from {filepath}")
        except Exception as e:
            logger.error(f"Failed to load system state: {e}")


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
async def demo_autonomous_evolutionary_system():
    """æ¼”ç¤ºè‡ªä¸»è¿›åŒ–ç³»ç»Ÿ"""
    print("ğŸš€ å¯åŠ¨è‡ªä¸»è¿›åŒ–Agentç³»ç»Ÿæ¼”ç¤º...")
    
    # åˆ›å»ºç³»ç»Ÿ
    system = AutonomousEvolutionarySystem()
    
    # åˆ›å»ºæ ‡å‡†å›¢é˜Ÿ
    team = system.create_standard_team()
    
    print(f"âœ… åˆ›å»ºäº†åŒ…å« {len(team)} ä¸ªAgentçš„å›¢é˜Ÿï¼š")
    for role, agent in team.items():
        print(f"   - {role}: {agent.agent_id}")
    
    # è¿è¡Œåä½œä»»åŠ¡
    goals = [
        "ç ”ç©¶å¹¶å®ç°ä¸€ä¸ªæ–°çš„æœºå™¨å­¦ä¹ ç®—æ³•",
        "ä¼˜åŒ–ç°æœ‰ç³»ç»Ÿçš„æ€§èƒ½",
        "è®¾è®¡ä¸€ä¸ªè‡ªåŠ¨åŒ–æµ‹è¯•æ¡†æ¶"
    ]
    
    for i, goal in enumerate(goals, 1):
        print(f"\nğŸ¯ ä»»åŠ¡ {i}: {goal}")
        result = await system.run_collaborative_task(goal, max_cycles=3)
        
        print(f"ğŸ“Š ä»»åŠ¡ç»“æœ:")
        print(f"   - æ€»å¾ªç¯æ•°: {result['total_cycles']}")
        print(f"   - æ€»è¡ŒåŠ¨æ•°: {result['total_actions']}")
        print(f"   - è¿›åŒ–å¾ªç¯æ•°: {result['evolution_cycles']}")
        
        if result['final_metrics']:
            metrics = result['final_metrics']
            print(f"   - æœ€ç»ˆæ€§èƒ½: {metrics.composite_score:.3f}")
            print(f"   - å¯è®­ç»ƒæ€§: {metrics.trainability:.3f}")
            print(f"   - æ³›åŒ–èƒ½åŠ›: {metrics.generalization:.3f}")
            print(f"   - è¡¨è¾¾èƒ½åŠ›: {metrics.expressiveness:.3f}")
    
    # ä¿å­˜ç³»ç»ŸçŠ¶æ€
    system.save_system_state("autonomous_system_state.pkl")
    print("\nğŸ’¾ ç³»ç»ŸçŠ¶æ€å·²ä¿å­˜")
    
    # æ˜¾ç¤ºç³»ç»Ÿè¿›åŒ–å†å²
    print(f"\nğŸ“ˆ ç³»ç»Ÿè¿›åŒ–å†å² ({len(system.system_metrics)} ä¸ªè¯„ä¼°ç‚¹):")
    for i, record in enumerate(system.system_metrics[-5:], 1):  # æ˜¾ç¤ºæœ€è¿‘5ä¸ªè¯„ä¼°ç‚¹
        metrics = record['metrics']
        print(f"   è¯„ä¼° {i}: ç»¼åˆå¾—åˆ† {metrics.composite_score:.3f}")
    
    print("\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")


if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    asyncio.run(demo_autonomous_evolutionary_system())