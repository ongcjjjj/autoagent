# è‡ªä¸»è¿›åŒ–Agent - ç¬¬2è½®æå‡ï¼šå¤šæ¨¡æ€æ„ŸçŸ¥ç³»ç»Ÿ
# Multimodal Perception System - ç»Ÿä¸€å¤„ç†æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘

import asyncio
import numpy as np
from typing import Dict, List, Tuple, Any, Optional, Union
from dataclasses import dataclass
from enum import Enum
import json
import time
import base64
import hashlib
from collections import defaultdict
import logging
from abc import ABC, abstractmethod

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ModalityType(Enum):
    """æ¨¡æ€ç±»å‹æšä¸¾"""
    TEXT = "text"
    IMAGE = "image"
    AUDIO = "audio"
    VIDEO = "video"
    SENSOR = "sensor"
    TEMPORAL = "temporal"
    SPATIAL = "spatial"

class PerceptionConfidence(Enum):
    """æ„ŸçŸ¥ç½®ä¿¡åº¦ç­‰çº§"""
    VERY_HIGH = 0.95
    HIGH = 0.8
    MEDIUM = 0.6
    LOW = 0.4
    VERY_LOW = 0.2

@dataclass
class PerceptionResult:
    """æ„ŸçŸ¥ç»“æœæ•°æ®ç»“æ„"""
    modality: ModalityType
    content: Any
    confidence: float
    features: Dict[str, Any]
    timestamp: float
    metadata: Dict[str, Any]
    embeddings: Optional[List[float]] = None

@dataclass
class CrossModalMapping:
    """è·¨æ¨¡æ€æ˜ å°„"""
    source_modality: ModalityType
    target_modality: ModalityType
    mapping_function: str
    confidence: float
    semantic_similarity: float

class TextPerceptionEngine:
    """æ–‡æœ¬æ„ŸçŸ¥å¼•æ“"""
    
    def __init__(self):
        self.vocabulary = set()
        self.semantic_cache = {}
        self.language_models = {}
        self.entity_extractors = {}
        
    def extract_features(self, text: str) -> Dict[str, Any]:
        """æå–æ–‡æœ¬ç‰¹å¾"""
        features = {
            'length': len(text),
            'word_count': len(text.split()),
            'sentence_count': text.count('.') + text.count('!') + text.count('?'),
            'avg_word_length': np.mean([len(word) for word in text.split()]) if text.split() else 0,
            'language': self.detect_language(text),
            'sentiment': self.analyze_sentiment(text),
            'entities': self.extract_entities(text),
            'keywords': self.extract_keywords(text),
            'topics': self.extract_topics(text),
            'complexity': self.calculate_complexity(text)
        }
        return features
        
    def detect_language(self, text: str) -> str:
        """è¯­è¨€æ£€æµ‹ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # åŸºäºå­—ç¬¦æ¨¡å¼çš„ç®€å•è¯­è¨€æ£€æµ‹
        chinese_chars = len([c for c in text if '\u4e00' <= c <= '\u9fff'])
        english_chars = len([c for c in text if c.isalpha() and ord(c) < 128])
        
        if chinese_chars > english_chars:
            return "zh"
        elif english_chars > 0:
            return "en"
        else:
            return "unknown"
            
    def analyze_sentiment(self, text: str) -> Dict[str, float]:
        """æƒ…æ„Ÿåˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        positive_words = ['å¥½', 'ä¼˜ç§€', 'æ£’', 'excellent', 'good', 'great', 'amazing']
        negative_words = ['å', 'å·®', 'ç³Ÿç³•', 'bad', 'terrible', 'awful', 'poor']
        
        words = text.lower().split()
        positive_score = sum(1 for word in words if word in positive_words)
        negative_score = sum(1 for word in words if word in negative_words)
        
        total_score = positive_score + negative_score
        if total_score == 0:
            return {"positive": 0.5, "negative": 0.5, "neutral": 1.0}
            
        return {
            "positive": positive_score / total_score,
            "negative": negative_score / total_score,
            "neutral": 1.0 - (positive_score + negative_score) / len(words)
        }
        
    def extract_entities(self, text: str) -> List[Dict[str, str]]:
        """å®ä½“æå–ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        entities = []
        words = text.split()
        
        for i, word in enumerate(words):
            if word.istitle() and len(word) > 2:
                entities.append({
                    "text": word,
                    "type": "PERSON" if any(c.isupper() for c in word) else "ORGANIZATION",
                    "position": i
                })
                
        return entities
        
    def extract_keywords(self, text: str) -> List[str]:
        """å…³é”®è¯æå–ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        stop_words = {'çš„', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'the', 'is', 'in', 'and', 'of', 'to', 'a'}
        words = [word.lower() for word in text.split() if word.lower() not in stop_words and len(word) > 2]
        
        # ç®€å•çš„é¢‘ç‡ç»Ÿè®¡
        word_freq = defaultdict(int)
        for word in words:
            word_freq[word] += 1
            
        # è¿”å›é¢‘ç‡æœ€é«˜çš„å‰10ä¸ªè¯
        return sorted(word_freq.keys(), key=lambda x: word_freq[x], reverse=True)[:10]
        
    def extract_topics(self, text: str) -> List[str]:
        """ä¸»é¢˜æå–ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        topic_keywords = {
            "technology": ["AI", "æœºå™¨å­¦ä¹ ", "äººå·¥æ™ºèƒ½", "ç®—æ³•", "technology", "computer"],
            "business": ["å…¬å¸", "å•†ä¸š", "è¥é”€", "business", "company", "market"],
            "science": ["ç§‘å­¦", "ç ”ç©¶", "å®éªŒ", "science", "research", "experiment"],
            "education": ["æ•™è‚²", "å­¦ä¹ ", "è¯¾ç¨‹", "education", "learning", "course"]
        }
        
        text_lower = text.lower()
        topics = []
        
        for topic, keywords in topic_keywords.items():
            if any(keyword.lower() in text_lower for keyword in keywords):
                topics.append(topic)
                
        return topics if topics else ["general"]
        
    def calculate_complexity(self, text: str) -> float:
        """è®¡ç®—æ–‡æœ¬å¤æ‚åº¦"""
        words = text.split()
        if not words:
            return 0.0
            
        avg_word_length = np.mean([len(word) for word in words])
        sentence_count = max(1, text.count('.') + text.count('!') + text.count('?'))
        words_per_sentence = len(words) / sentence_count
        
        # å¤æ‚åº¦ç»¼åˆè¯„åˆ†
        complexity = (avg_word_length * 0.3 + words_per_sentence * 0.7) / 10
        return min(complexity, 1.0)
        
    def perceive(self, text: str) -> PerceptionResult:
        """æ–‡æœ¬æ„ŸçŸ¥ä¸»å‡½æ•°"""
        features = self.extract_features(text)
        
        # è®¡ç®—ç½®ä¿¡åº¦
        confidence = PerceptionConfidence.HIGH.value
        if features['length'] < 10:
            confidence = PerceptionConfidence.LOW.value
        elif features['word_count'] > 100:
            confidence = PerceptionConfidence.VERY_HIGH.value
            
        return PerceptionResult(
            modality=ModalityType.TEXT,
            content=text,
            confidence=confidence,
            features=features,
            timestamp=time.time(),
            metadata={
                "processing_method": "text_perception_engine",
                "version": "2.0"
            }
        )

class ImagePerceptionEngine:
    """å›¾åƒæ„ŸçŸ¥å¼•æ“"""
    
    def __init__(self):
        self.color_extractors = {}
        self.shape_detectors = {}
        self.object_classifiers = {}
        
    def extract_features(self, image_data: Any) -> Dict[str, Any]:
        """æå–å›¾åƒç‰¹å¾ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        # è¿™é‡Œæ¨¡æ‹Ÿå›¾åƒç‰¹å¾æå–
        features = {
            'dimensions': (224, 224, 3),  # å‡è®¾çš„å›¾åƒå°ºå¯¸
            'color_histogram': self.extract_color_histogram(),
            'edges': self.detect_edges(),
            'textures': self.analyze_textures(),
            'objects': self.detect_objects(),
            'faces': self.detect_faces(),
            'scene_type': self.classify_scene(),
            'aesthetic_score': self.calculate_aesthetic_score(),
            'complexity': self.calculate_visual_complexity()
        }
        return features
        
    def extract_color_histogram(self) -> Dict[str, int]:
        """é¢œè‰²ç›´æ–¹å›¾ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        return {
            'red': np.random.randint(0, 255),
            'green': np.random.randint(0, 255),
            'blue': np.random.randint(0, 255),
            'dominant_color': ['red', 'green', 'blue'][np.random.randint(0, 3)]
        }
        
    def detect_edges(self) -> Dict[str, Any]:
        """è¾¹ç¼˜æ£€æµ‹ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        return {
            'edge_density': np.random.uniform(0.1, 0.9),
            'dominant_orientation': np.random.choice(['horizontal', 'vertical', 'diagonal']),
            'sharpness': np.random.uniform(0.0, 1.0)
        }
        
    def analyze_textures(self) -> Dict[str, float]:
        """çº¹ç†åˆ†æï¼ˆæ¨¡æ‹Ÿï¼‰"""
        return {
            'smoothness': np.random.uniform(0.0, 1.0),
            'roughness': np.random.uniform(0.0, 1.0),
            'regularity': np.random.uniform(0.0, 1.0),
            'contrast': np.random.uniform(0.0, 1.0)
        }
        
    def detect_objects(self) -> List[Dict[str, Any]]:
        """ç‰©ä½“æ£€æµ‹ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        possible_objects = ['person', 'car', 'building', 'tree', 'animal', 'book', 'computer']
        num_objects = np.random.randint(1, 4)
        
        objects = []
        for _ in range(num_objects):
            objects.append({
                'class': np.random.choice(possible_objects),
                'confidence': np.random.uniform(0.5, 0.95),
                'bbox': [np.random.randint(0, 200) for _ in range(4)]
            })
        return objects
        
    def detect_faces(self) -> List[Dict[str, Any]]:
        """äººè„¸æ£€æµ‹ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        num_faces = np.random.randint(0, 3)
        faces = []
        
        for _ in range(num_faces):
            faces.append({
                'confidence': np.random.uniform(0.7, 0.98),
                'age_estimate': np.random.randint(18, 80),
                'gender_estimate': np.random.choice(['male', 'female']),
                'emotion': np.random.choice(['happy', 'sad', 'neutral', 'surprised'])
            })
        return faces
        
    def classify_scene(self) -> str:
        """åœºæ™¯åˆ†ç±»ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        scenes = ['indoor', 'outdoor', 'urban', 'nature', 'office', 'home', 'street']
        return np.random.choice(scenes)
        
    def calculate_aesthetic_score(self) -> float:
        """ç¾å­¦è¯„åˆ†ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        return np.random.uniform(0.3, 0.9)
        
    def calculate_visual_complexity(self) -> float:
        """è§†è§‰å¤æ‚åº¦ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        return np.random.uniform(0.2, 0.8)
        
    def perceive(self, image_data: Any) -> PerceptionResult:
        """å›¾åƒæ„ŸçŸ¥ä¸»å‡½æ•°"""
        features = self.extract_features(image_data)
        
        # åŸºäºç‰¹å¾è®¡ç®—ç½®ä¿¡åº¦
        confidence = PerceptionConfidence.HIGH.value
        if features['objects']:
            avg_object_confidence = np.mean([obj['confidence'] for obj in features['objects']])
            confidence = avg_object_confidence
            
        return PerceptionResult(
            modality=ModalityType.IMAGE,
            content=image_data,
            confidence=confidence,
            features=features,
            timestamp=time.time(),
            metadata={
                "processing_method": "image_perception_engine",
                "version": "2.0"
            }
        )

class AudioPerceptionEngine:
    """éŸ³é¢‘æ„ŸçŸ¥å¼•æ“"""
    
    def __init__(self):
        self.speech_recognizers = {}
        self.audio_classifiers = {}
        self.emotion_analyzers = {}
        
    def extract_features(self, audio_data: Any) -> Dict[str, Any]:
        """æå–éŸ³é¢‘ç‰¹å¾ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        features = {
            'duration': np.random.uniform(1.0, 30.0),
            'sample_rate': 44100,
            'channels': np.random.choice([1, 2]),
            'spectral_features': self.extract_spectral_features(),
            'temporal_features': self.extract_temporal_features(),
            'speech_detection': self.detect_speech(),
            'music_detection': self.detect_music(),
            'emotion_analysis': self.analyze_audio_emotion(),
            'energy_level': np.random.uniform(0.1, 0.9),
            'pitch_analysis': self.analyze_pitch()
        }
        return features
        
    def extract_spectral_features(self) -> Dict[str, float]:
        """é¢‘è°±ç‰¹å¾ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        return {
            'spectral_centroid': np.random.uniform(1000, 4000),
            'spectral_bandwidth': np.random.uniform(500, 2000),
            'spectral_rolloff': np.random.uniform(2000, 8000),
            'zero_crossing_rate': np.random.uniform(0.05, 0.3),
            'mfcc_coefficients': np.random.uniform(-10, 10, 13).tolist()
        }
        
    def extract_temporal_features(self) -> Dict[str, float]:
        """æ—¶åŸŸç‰¹å¾ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        return {
            'rms_energy': np.random.uniform(0.01, 0.5),
            'tempo': np.random.uniform(60, 180),
            'rhythm_regularity': np.random.uniform(0.3, 0.9),
            'silence_ratio': np.random.uniform(0.0, 0.4)
        }
        
    def detect_speech(self) -> Dict[str, Any]:
        """è¯­éŸ³æ£€æµ‹ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        has_speech = np.random.choice([True, False], p=[0.7, 0.3])
        
        if has_speech:
            return {
                'has_speech': True,
                'speech_confidence': np.random.uniform(0.7, 0.95),
                'language': np.random.choice(['zh', 'en', 'unknown']),
                'speaker_count': np.random.randint(1, 4),
                'transcription': "è¿™æ˜¯æ¨¡æ‹Ÿçš„è¯­éŸ³è¯†åˆ«ç»“æœ"
            }
        else:
            return {'has_speech': False}
            
    def detect_music(self) -> Dict[str, Any]:
        """éŸ³ä¹æ£€æµ‹ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        has_music = np.random.choice([True, False], p=[0.5, 0.5])
        
        if has_music:
            return {
                'has_music': True,
                'music_confidence': np.random.uniform(0.6, 0.9),
                'genre': np.random.choice(['classical', 'rock', 'pop', 'jazz', 'electronic']),
                'instruments': np.random.choice(['piano', 'guitar', 'violin', 'drums'], size=np.random.randint(1, 3)).tolist()
            }
        else:
            return {'has_music': False}
            
    def analyze_audio_emotion(self) -> Dict[str, float]:
        """éŸ³é¢‘æƒ…æ„Ÿåˆ†æï¼ˆæ¨¡æ‹Ÿï¼‰"""
        emotions = ['happy', 'sad', 'angry', 'neutral', 'excited', 'calm']
        emotion_scores = np.random.dirichlet(np.ones(len(emotions)))
        
        return {emotion: float(score) for emotion, score in zip(emotions, emotion_scores)}
        
    def analyze_pitch(self) -> Dict[str, float]:
        """éŸ³è°ƒåˆ†æï¼ˆæ¨¡æ‹Ÿï¼‰"""
        return {
            'fundamental_frequency': np.random.uniform(80, 400),
            'pitch_stability': np.random.uniform(0.3, 0.9),
            'pitch_range': np.random.uniform(50, 300),
            'intonation_pattern': np.random.choice(['rising', 'falling', 'flat', 'varying'])
        }
        
    def perceive(self, audio_data: Any) -> PerceptionResult:
        """éŸ³é¢‘æ„ŸçŸ¥ä¸»å‡½æ•°"""
        features = self.extract_features(audio_data)
        
        # è®¡ç®—ç½®ä¿¡åº¦
        confidence = PerceptionConfidence.MEDIUM.value
        if features['speech_detection'].get('has_speech'):
            confidence = features['speech_detection']['speech_confidence']
        elif features['music_detection'].get('has_music'):
            confidence = features['music_detection']['music_confidence']
            
        return PerceptionResult(
            modality=ModalityType.AUDIO,
            content=audio_data,
            confidence=confidence,
            features=features,
            timestamp=time.time(),
            metadata={
                "processing_method": "audio_perception_engine",
                "version": "2.0"
            }
        )

class CrossModalFusionEngine:
    """è·¨æ¨¡æ€èåˆå¼•æ“"""
    
    def __init__(self):
        self.fusion_strategies = {}
        self.semantic_alignments = {}
        self.cross_modal_mappings = []
        
    def add_cross_modal_mapping(self, mapping: CrossModalMapping):
        """æ·»åŠ è·¨æ¨¡æ€æ˜ å°„"""
        self.cross_modal_mappings.append(mapping)
        logger.info(f"æ·»åŠ è·¨æ¨¡æ€æ˜ å°„: {mapping.source_modality.value} -> {mapping.target_modality.value}")
        
    def semantic_alignment(self, results: List[PerceptionResult]) -> Dict[str, Any]:
        """è¯­ä¹‰å¯¹é½"""
        aligned_semantics = {
            'common_concepts': [],
            'cross_modal_consistency': 0.0,
            'semantic_richness': 0.0,
            'unified_representation': {}
        }
        
        # æå–å…±åŒæ¦‚å¿µ
        text_concepts = set()
        image_concepts = set()
        audio_concepts = set()
        
        for result in results:
            if result.modality == ModalityType.TEXT:
                text_concepts.update(result.features.get('keywords', []))
                text_concepts.update(result.features.get('topics', []))
            elif result.modality == ModalityType.IMAGE:
                image_concepts.update([obj['class'] for obj in result.features.get('objects', [])])
                image_concepts.add(result.features.get('scene_type', ''))
            elif result.modality == ModalityType.AUDIO:
                if result.features.get('music_detection', {}).get('has_music'):
                    audio_concepts.add('music')
                    audio_concepts.update(result.features['music_detection'].get('instruments', []))
                if result.features.get('speech_detection', {}).get('has_speech'):
                    audio_concepts.add('speech')
                    
        # å¯»æ‰¾å…±åŒæ¦‚å¿µ
        all_concepts = text_concepts | image_concepts | audio_concepts
        common_concepts = []
        
        for concept in all_concepts:
            modality_count = 0
            if concept in text_concepts:
                modality_count += 1
            if concept in image_concepts:
                modality_count += 1
            if concept in audio_concepts:
                modality_count += 1
                
            if modality_count > 1:
                common_concepts.append(concept)
                
        aligned_semantics['common_concepts'] = common_concepts
        aligned_semantics['cross_modal_consistency'] = len(common_concepts) / max(len(all_concepts), 1)
        aligned_semantics['semantic_richness'] = len(all_concepts) / len(results) if results else 0
        
        return aligned_semantics
        
    def temporal_synchronization(self, results: List[PerceptionResult]) -> Dict[str, Any]:
        """æ—¶åºåŒæ­¥"""
        if not results:
            return {'synchronized': False}
            
        timestamps = [result.timestamp for result in results]
        time_window = max(timestamps) - min(timestamps)
        
        return {
            'synchronized': time_window < 1.0,  # 1ç§’å†…è®¤ä¸ºæ˜¯åŒæ­¥çš„
            'time_window': time_window,
            'temporal_order': sorted(results, key=lambda x: x.timestamp)
        }
        
    def confidence_weighting(self, results: List[PerceptionResult]) -> Dict[str, float]:
        """ç½®ä¿¡åº¦åŠ æƒ"""
        if not results:
            return {}
            
        total_confidence = sum(result.confidence for result in results)
        weights = {}
        
        for result in results:
            weights[result.modality.value] = result.confidence / total_confidence if total_confidence > 0 else 0
            
        return weights
        
    def fuse_multimodal(self, results: List[PerceptionResult]) -> Dict[str, Any]:
        """å¤šæ¨¡æ€èåˆ"""
        if not results:
            return {}
            
        fusion_result = {
            'input_modalities': [result.modality.value for result in results],
            'semantic_alignment': self.semantic_alignment(results),
            'temporal_sync': self.temporal_synchronization(results),
            'confidence_weights': self.confidence_weighting(results),
            'overall_confidence': np.mean([result.confidence for result in results]),
            'fusion_timestamp': time.time(),
            'cross_modal_features': self.extract_cross_modal_features(results)
        }
        
        return fusion_result
        
    def extract_cross_modal_features(self, results: List[PerceptionResult]) -> Dict[str, Any]:
        """æå–è·¨æ¨¡æ€ç‰¹å¾"""
        cross_features = {
            'modality_diversity': len(set(result.modality for result in results)),
            'feature_complexity': 0.0,
            'information_density': 0.0,
            'semantic_coherence': 0.0
        }
        
        # è®¡ç®—ç‰¹å¾å¤æ‚åº¦
        total_features = 0
        for result in results:
            total_features += len(result.features)
            
        cross_features['feature_complexity'] = total_features / len(results) if results else 0
        
        # è®¡ç®—ä¿¡æ¯å¯†åº¦
        content_lengths = []
        for result in results:
            if result.modality == ModalityType.TEXT:
                content_lengths.append(len(str(result.content)))
            else:
                content_lengths.append(100)  # é»˜è®¤å€¼foréæ–‡æœ¬æ¨¡æ€
                
        cross_features['information_density'] = np.mean(content_lengths) if content_lengths else 0
        
        # è®¡ç®—è¯­ä¹‰ä¸€è‡´æ€§
        semantic_alignment = self.semantic_alignment(results)
        cross_features['semantic_coherence'] = semantic_alignment['cross_modal_consistency']
        
        return cross_features

class MultimodalPerceptionSystem:
    """å¤šæ¨¡æ€æ„ŸçŸ¥ç³»ç»Ÿä¸»æ§åˆ¶å™¨"""
    
    def __init__(self):
        self.text_engine = TextPerceptionEngine()
        self.image_engine = ImagePerceptionEngine()
        self.audio_engine = AudioPerceptionEngine()
        self.fusion_engine = CrossModalFusionEngine()
        
        self.perception_history = []
        self.performance_metrics = {
            'total_perceptions': 0,
            'successful_fusions': 0,
            'modality_distribution': defaultdict(int),
            'avg_confidence': 0.0,
            'processing_times': []
        }
        
        # åˆå§‹åŒ–è·¨æ¨¡æ€æ˜ å°„
        self._initialize_cross_modal_mappings()
        
    def _initialize_cross_modal_mappings(self):
        """åˆå§‹åŒ–è·¨æ¨¡æ€æ˜ å°„"""
        mappings = [
            CrossModalMapping(
                source_modality=ModalityType.TEXT,
                target_modality=ModalityType.IMAGE,
                mapping_function="text_to_image_generation",
                confidence=0.7,
                semantic_similarity=0.8
            ),
            CrossModalMapping(
                source_modality=ModalityType.AUDIO,
                target_modality=ModalityType.TEXT,
                mapping_function="speech_to_text",
                confidence=0.9,
                semantic_similarity=0.95
            ),
            CrossModalMapping(
                source_modality=ModalityType.IMAGE,
                target_modality=ModalityType.TEXT,
                mapping_function="image_captioning",
                confidence=0.8,
                semantic_similarity=0.85
            )
        ]
        
        for mapping in mappings:
            self.fusion_engine.add_cross_modal_mapping(mapping)
            
    def perceive_multimodal(self, inputs: Dict[ModalityType, Any]) -> Dict[str, Any]:
        """å¤šæ¨¡æ€æ„ŸçŸ¥ä¸»å‡½æ•°"""
        start_time = time.time()
        perception_results = []
        
        # å„æ¨¡æ€ç‹¬ç«‹æ„ŸçŸ¥
        for modality, data in inputs.items():
            try:
                if modality == ModalityType.TEXT:
                    result = self.text_engine.perceive(data)
                elif modality == ModalityType.IMAGE:
                    result = self.image_engine.perceive(data)
                elif modality == ModalityType.AUDIO:
                    result = self.audio_engine.perceive(data)
                else:
                    logger.warning(f"æœªæ”¯æŒçš„æ¨¡æ€ç±»å‹: {modality}")
                    continue
                    
                perception_results.append(result)
                self.performance_metrics['modality_distribution'][modality.value] += 1
                
            except Exception as e:
                logger.error(f"æ„ŸçŸ¥é”™è¯¯ - {modality.value}: {str(e)}")
                continue
                
        # è·¨æ¨¡æ€èåˆ
        fusion_result = self.fusion_engine.fuse_multimodal(perception_results)
        
        # ç”Ÿæˆç»¼åˆæ„ŸçŸ¥ç»“æœ
        comprehensive_result = {
            'individual_perceptions': perception_results,
            'fusion_result': fusion_result,
            'processing_time': time.time() - start_time,
            'timestamp': time.time(),
            'metadata': {
                'version': '2.0',
                'modalities_processed': len(inputs),
                'fusion_strategy': 'weighted_semantic_alignment'
            }
        }
        
        # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
        self._update_metrics(comprehensive_result)
        
        # ä¿å­˜åˆ°å†å²è®°å½•
        self.perception_history.append(comprehensive_result)
        
        return comprehensive_result
        
    def _update_metrics(self, result: Dict[str, Any]):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
        self.performance_metrics['total_perceptions'] += 1
        
        if result['fusion_result']:
            self.performance_metrics['successful_fusions'] += 1
            
        # æ›´æ–°å¹³å‡ç½®ä¿¡åº¦
        if result['individual_perceptions']:
            avg_confidence = np.mean([p.confidence for p in result['individual_perceptions']])
            self.performance_metrics['avg_confidence'] = (
                self.performance_metrics['avg_confidence'] * (self.performance_metrics['total_perceptions'] - 1) +
                avg_confidence
            ) / self.performance_metrics['total_perceptions']
            
        # è®°å½•å¤„ç†æ—¶é—´
        self.performance_metrics['processing_times'].append(result['processing_time'])
        
    def get_perception_summary(self, result: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ„ŸçŸ¥ç»“æœæ‘˜è¦"""
        summary = "ğŸ” å¤šæ¨¡æ€æ„ŸçŸ¥ç»“æœæ‘˜è¦\n"
        summary += "=" * 40 + "\n"
        
        # ä¸ªä½“æ„ŸçŸ¥ç»“æœ
        summary += "ğŸ“Š å„æ¨¡æ€æ„ŸçŸ¥ç»“æœ:\n"
        for perception in result['individual_perceptions']:
            summary += f"  {perception.modality.value}: ç½®ä¿¡åº¦ {perception.confidence:.3f}\n"
            
        # èåˆç»“æœ
        fusion = result['fusion_result']
        if fusion:
            summary += "\nğŸ”— è·¨æ¨¡æ€èåˆ:\n"
            summary += f"  è¾“å…¥æ¨¡æ€: {', '.join(fusion['input_modalities'])}\n"
            summary += f"  æ•´ä½“ç½®ä¿¡åº¦: {fusion['overall_confidence']:.3f}\n"
            summary += f"  è¯­ä¹‰ä¸€è‡´æ€§: {fusion['semantic_alignment']['cross_modal_consistency']:.3f}\n"
            summary += f"  å…±åŒæ¦‚å¿µ: {', '.join(fusion['semantic_alignment']['common_concepts'])}\n"
            
        # å¤„ç†æ€§èƒ½
        summary += f"\nâ±ï¸  å¤„ç†æ—¶é—´: {result['processing_time']:.3f}ç§’\n"
        
        return summary
        
    def get_performance_report(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        success_rate = (
            self.performance_metrics['successful_fusions'] / 
            max(self.performance_metrics['total_perceptions'], 1)
        )
        
        avg_processing_time = (
            np.mean(self.performance_metrics['processing_times']) 
            if self.performance_metrics['processing_times'] else 0
        )
        
        return {
            "æ€»æ„ŸçŸ¥æ¬¡æ•°": self.performance_metrics['total_perceptions'],
            "æˆåŠŸèåˆæ¬¡æ•°": self.performance_metrics['successful_fusions'],
            "èåˆæˆåŠŸç‡": f"{success_rate:.2%}",
            "å¹³å‡ç½®ä¿¡åº¦": f"{self.performance_metrics['avg_confidence']:.3f}",
            "å¹³å‡å¤„ç†æ—¶é—´": f"{avg_processing_time:.3f}ç§’",
            "æ¨¡æ€åˆ†å¸ƒ": dict(self.performance_metrics['modality_distribution']),
            "å†å²è®°å½•æ•°": len(self.perception_history)
        }

# ç¤ºä¾‹ä½¿ç”¨å’Œæµ‹è¯•
async def demonstrate_multimodal_perception():
    """æ¼”ç¤ºå¤šæ¨¡æ€æ„ŸçŸ¥ç³»ç»ŸåŠŸèƒ½"""
    print("ğŸŒŸ è‡ªä¸»è¿›åŒ–Agent - ç¬¬2è½®æå‡ï¼šå¤šæ¨¡æ€æ„ŸçŸ¥ç³»ç»Ÿ")
    print("=" * 60)
    
    # åˆ›å»ºå¤šæ¨¡æ€æ„ŸçŸ¥ç³»ç»Ÿ
    perception_system = MultimodalPerceptionSystem()
    
    # ç¤ºä¾‹1: æ–‡æœ¬+å›¾åƒæ„ŸçŸ¥
    print("\nğŸ“–ğŸ–¼ï¸ ç¤ºä¾‹1: æ–‡æœ¬+å›¾åƒå¤šæ¨¡æ€æ„ŸçŸ¥")
    multimodal_input1 = {
        ModalityType.TEXT: "ä¸€åªå¯çˆ±çš„å°çŒ«åœ¨é˜³å…‰ä¸‹ç¡è§‰",
        ModalityType.IMAGE: "cat_image_data"  # æ¨¡æ‹Ÿå›¾åƒæ•°æ®
    }
    
    result1 = perception_system.perceive_multimodal(multimodal_input1)
    print(perception_system.get_perception_summary(result1))
    
    # ç¤ºä¾‹2: æ–‡æœ¬+éŸ³é¢‘æ„ŸçŸ¥
    print("\nğŸ“–ğŸµ ç¤ºä¾‹2: æ–‡æœ¬+éŸ³é¢‘å¤šæ¨¡æ€æ„ŸçŸ¥")
    multimodal_input2 = {
        ModalityType.TEXT: "è¿™æ˜¯ä¸€æ®µä¼˜ç¾çš„å¤å…¸éŸ³ä¹",
        ModalityType.AUDIO: "classical_music_data"  # æ¨¡æ‹ŸéŸ³é¢‘æ•°æ®
    }
    
    result2 = perception_system.perceive_multimodal(multimodal_input2)
    print(perception_system.get_perception_summary(result2))
    
    # ç¤ºä¾‹3: ä¸‰æ¨¡æ€èåˆæ„ŸçŸ¥
    print("\nğŸ“–ğŸ–¼ï¸ğŸµ ç¤ºä¾‹3: æ–‡æœ¬+å›¾åƒ+éŸ³é¢‘ä¸‰æ¨¡æ€æ„ŸçŸ¥")
    multimodal_input3 = {
        ModalityType.TEXT: "éŸ³ä¹ä¼šä¸Šçš„é’¢ç´æ¼”å¥",
        ModalityType.IMAGE: "concert_image_data",
        ModalityType.AUDIO: "piano_performance_data"
    }
    
    result3 = perception_system.perceive_multimodal(multimodal_input3)
    print(perception_system.get_perception_summary(result3))
    
    # æ€§èƒ½æŠ¥å‘Š
    print("\nğŸ“Š ç³»ç»Ÿæ€§èƒ½æŠ¥å‘Š")
    report = perception_system.get_performance_report()
    for key, value in report.items():
        print(f"{key}: {value}")
        
    print("\nâœ… ç¬¬2è½®æå‡å®Œæˆï¼å¤šæ¨¡æ€æ„ŸçŸ¥ç³»ç»Ÿå·²æˆåŠŸéƒ¨ç½²")

if __name__ == "__main__":
    asyncio.run(demonstrate_multimodal_perception())