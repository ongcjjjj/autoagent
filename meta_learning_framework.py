# è‡ªä¸»è¿›åŒ–Agent - ç¬¬3è½®æå‡ï¼šå…ƒå­¦ä¹ æ¡†æ¶
# Meta Learning Framework - Few-shotå­¦ä¹ ä¸å¿«é€Ÿé€‚åº”èƒ½åŠ›

import asyncio
import math
import random
from typing import Dict, List, Tuple, Any, Optional, Union, Callable
from dataclasses import dataclass, field
from enum import Enum
import json
import time
from collections import defaultdict, deque
import logging
from abc import ABC, abstractmethod
import copy

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MetaLearningStrategy(Enum):
    """å…ƒå­¦ä¹ ç­–ç•¥æšä¸¾"""
    MODEL_AGNOSTIC = "model_agnostic"           # æ¨¡å‹æ— å…³å…ƒå­¦ä¹  (MAML)
    GRADIENT_BASED = "gradient_based"           # åŸºäºæ¢¯åº¦çš„å…ƒå­¦ä¹ 
    MEMORY_AUGMENTED = "memory_augmented"       # è®°å¿†å¢å¼ºç½‘ç»œ
    PROTOTYPE_BASED = "prototype_based"         # åŸå‹ç½‘ç»œ
    RELATION_BASED = "relation_based"           # å…³ç³»ç½‘ç»œ
    OPTIMIZATION_BASED = "optimization_based"   # åŸºäºä¼˜åŒ–çš„å…ƒå­¦ä¹ 
    METRIC_LEARNING = "metric_learning"         # åº¦é‡å­¦ä¹ 
    TRANSFER_LEARNING = "transfer_learning"     # è¿ç§»å­¦ä¹ 

class TaskType(Enum):
    """ä»»åŠ¡ç±»å‹æšä¸¾"""
    CLASSIFICATION = "classification"
    REGRESSION = "regression"
    SEQUENCE_MODELING = "sequence_modeling"
    REINFORCEMENT_LEARNING = "rl"
    GENERATION = "generation"
    REASONING = "reasoning"
    PLANNING = "planning"
    MULTIMODAL = "multimodal"

@dataclass
class Task:
    """ä»»åŠ¡æ•°æ®ç»“æ„"""
    task_id: str
    task_type: TaskType
    domain: str
    description: str
    support_set: List[Tuple[Any, Any]]  # (input, output) pairs
    query_set: List[Tuple[Any, Any]]
    metadata: Dict[str, Any] = field(default_factory=dict)
    difficulty: float = 0.5
    similarity_features: List[float] = field(default_factory=list)

@dataclass
class MetaKnowledge:
    """å…ƒçŸ¥è¯†æ•°æ®ç»“æ„"""
    knowledge_id: str
    knowledge_type: str
    content: Any
    applicability_score: float
    usage_count: int = 0
    success_rate: float = 0.0
    last_updated: float = field(default_factory=time.time)
    source_tasks: List[str] = field(default_factory=list)

@dataclass
class LearningExperience:
    """å­¦ä¹ ç»éªŒæ•°æ®ç»“æ„"""
    task_id: str
    strategy_used: MetaLearningStrategy
    performance: float
    adaptation_steps: int
    learning_curve: List[float]
    insights: List[str]
    timestamp: float = field(default_factory=time.time)

class TaskSimilarityMeasurer:
    """ä»»åŠ¡ç›¸ä¼¼æ€§åº¦é‡å™¨"""
    
    def __init__(self):
        self.similarity_cache = {}
        self.feature_weights = {
            'domain': 0.3,
            'task_type': 0.25,
            'complexity': 0.2,
            'data_structure': 0.15,
            'semantics': 0.1
        }
        
    def compute_similarity(self, task1: Task, task2: Task) -> float:
        """è®¡ç®—ä¸¤ä¸ªä»»åŠ¡çš„ç›¸ä¼¼æ€§"""
        cache_key = f"{task1.task_id}_{task2.task_id}"
        if cache_key in self.similarity_cache:
            return self.similarity_cache[cache_key]
            
        similarity_scores = {}
        
        # 1. é¢†åŸŸç›¸ä¼¼æ€§
        similarity_scores['domain'] = 1.0 if task1.domain == task2.domain else 0.3
        
        # 2. ä»»åŠ¡ç±»å‹ç›¸ä¼¼æ€§
        similarity_scores['task_type'] = 1.0 if task1.task_type == task2.task_type else 0.2
        
        # 3. å¤æ‚åº¦ç›¸ä¼¼æ€§
        complexity_diff = abs(task1.difficulty - task2.difficulty)
        similarity_scores['complexity'] = max(0, 1.0 - complexity_diff)
        
        # 4. æ•°æ®ç»“æ„ç›¸ä¼¼æ€§
        similarity_scores['data_structure'] = self._compute_data_similarity(task1, task2)
        
        # 5. è¯­ä¹‰ç›¸ä¼¼æ€§
        similarity_scores['semantics'] = self._compute_semantic_similarity(task1, task2)
        
        # åŠ æƒæ€»åˆ†
        total_similarity = sum(
            self.feature_weights[feature] * score 
            for feature, score in similarity_scores.items()
        )
        
        self.similarity_cache[cache_key] = total_similarity
        return total_similarity
        
    def _compute_data_similarity(self, task1: Task, task2: Task) -> float:
        """è®¡ç®—æ•°æ®ç»“æ„ç›¸ä¼¼æ€§"""
        if not task1.support_set or not task2.support_set:
            return 0.5
            
        # ç®€åŒ–çš„æ•°æ®ç»“æ„ç›¸ä¼¼æ€§è®¡ç®—
        sample1 = task1.support_set[0]
        sample2 = task2.support_set[0]
        
        # æ¯”è¾ƒè¾“å…¥è¾“å‡ºçš„ç±»å‹
        input_type_match = type(sample1[0]) == type(sample2[0])
        output_type_match = type(sample1[1]) == type(sample2[1])
        
        similarity = 0.0
        if input_type_match:
            similarity += 0.5
        if output_type_match:
            similarity += 0.5
            
        return similarity
        
    def _compute_semantic_similarity(self, task1: Task, task2: Task) -> float:
        """è®¡ç®—è¯­ä¹‰ç›¸ä¼¼æ€§"""
        desc1_words = set(task1.description.lower().split())
        desc2_words = set(task2.description.lower().split())
        
        if not desc1_words or not desc2_words:
            return 0.5
            
        intersection = desc1_words & desc2_words
        union = desc1_words | desc2_words
        
        return len(intersection) / len(union) if union else 0.0
        
    def find_similar_tasks(self, target_task: Task, task_pool: List[Task], 
                          top_k: int = 5) -> List[Tuple[Task, float]]:
        """æ‰¾åˆ°æœ€ç›¸ä¼¼çš„kä¸ªä»»åŠ¡"""
        similarities = []
        for task in task_pool:
            if task.task_id != target_task.task_id:
                sim = self.compute_similarity(target_task, task)
                similarities.append((task, sim))
                
        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities[:top_k]

class MetaLearningAlgorithm(ABC):
    """å…ƒå­¦ä¹ ç®—æ³•æŠ½è±¡åŸºç±»"""
    
    @abstractmethod
    def meta_train(self, tasks: List[Task]) -> Dict[str, Any]:
        """å…ƒè®­ç»ƒè¿‡ç¨‹"""
        pass
        
    @abstractmethod
    def fast_adapt(self, new_task: Task, adaptation_steps: int = 5) -> Dict[str, Any]:
        """å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡"""
        pass
        
    @abstractmethod
    def evaluate_adaptation(self, task: Task, adapted_model: Any) -> float:
        """è¯„ä¼°é€‚åº”æ•ˆæœ"""
        pass

class ModelAgnosticMetaLearning(MetaLearningAlgorithm):
    """æ¨¡å‹æ— å…³å…ƒå­¦ä¹  (MAML)"""
    
    def __init__(self, learning_rate: float = 0.01, meta_learning_rate: float = 0.001):
        self.learning_rate = learning_rate
        self.meta_learning_rate = meta_learning_rate
        self.meta_parameters = {}
        self.adaptation_history = []
        
    def meta_train(self, tasks: List[Task]) -> Dict[str, Any]:
        """MAMLå…ƒè®­ç»ƒ"""
        logger.info(f"å¼€å§‹MAMLå…ƒè®­ç»ƒï¼Œä»»åŠ¡æ•°é‡: {len(tasks)}")
        
        meta_loss = 0.0
        meta_gradients = {}
        
        for task in tasks:
            # ä»»åŠ¡ç‰¹å®šçš„å¿«é€Ÿé€‚åº”
            adapted_params = self._inner_loop_adaptation(task)
            
            # è®¡ç®—å…ƒæŸå¤±
            task_loss = self._compute_meta_loss(task, adapted_params)
            meta_loss += task_loss
            
            # è®¡ç®—å…ƒæ¢¯åº¦
            task_gradients = self._compute_meta_gradients(task, adapted_params)
            for param_name, grad in task_gradients.items():
                if param_name not in meta_gradients:
                    meta_gradients[param_name] = []
                meta_gradients[param_name].append(grad)
                
        # æ›´æ–°å…ƒå‚æ•°
        for param_name, grad_list in meta_gradients.items():
            avg_gradient = sum(grad_list) / len(grad_list)
            if param_name not in self.meta_parameters:
                self.meta_parameters[param_name] = random.random()
            self.meta_parameters[param_name] -= self.meta_learning_rate * avg_gradient
            
        training_result = {
            'meta_loss': meta_loss / len(tasks),
            'meta_parameters': copy.deepcopy(self.meta_parameters),
            'tasks_trained': len(tasks),
            'strategy': MetaLearningStrategy.MODEL_AGNOSTIC
        }
        
        logger.info(f"MAMLå…ƒè®­ç»ƒå®Œæˆï¼Œå…ƒæŸå¤±: {training_result['meta_loss']:.4f}")
        return training_result
        
    def _inner_loop_adaptation(self, task: Task) -> Dict[str, Any]:
        """å†…å¾ªç¯å¿«é€Ÿé€‚åº”"""
        adapted_params = copy.deepcopy(self.meta_parameters)
        
        for step in range(5):  # å¿«é€Ÿé€‚åº”æ­¥æ•°
            # åœ¨æ”¯æŒé›†ä¸Šè®¡ç®—æ¢¯åº¦
            gradients = self._compute_task_gradients(task, adapted_params)
            
            # æ›´æ–°å‚æ•°
            for param_name, grad in gradients.items():
                adapted_params[param_name] -= self.learning_rate * grad
                
        return adapted_params
        
    def _compute_task_gradients(self, task: Task, params: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—ä»»åŠ¡ç‰¹å®šæ¢¯åº¦ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        gradients = {}
        
        # æ¨¡æ‹Ÿæ¢¯åº¦è®¡ç®—
        for param_name in params:
            # ç®€åŒ–çš„æ¢¯åº¦è®¡ç®—
            loss = self._compute_task_loss(task, params)
            gradients[param_name] = random.uniform(-0.1, 0.1) * loss
            
        return gradients
        
    def _compute_task_loss(self, task: Task, params: Dict[str, Any]) -> float:
        """è®¡ç®—ä»»åŠ¡æŸå¤±ï¼ˆæ¨¡æ‹Ÿï¼‰"""
        # åŸºäºä»»åŠ¡å¤æ‚åº¦å’Œå‚æ•°çš„æ¨¡æ‹ŸæŸå¤±
        base_loss = task.difficulty
        param_penalty = sum(abs(p) for p in params.values()) * 0.01
        return base_loss + param_penalty + random.uniform(0, 0.1)
        
    def _compute_meta_loss(self, task: Task, adapted_params: Dict[str, Any]) -> float:
        """è®¡ç®—å…ƒæŸå¤±"""
        # åœ¨æŸ¥è¯¢é›†ä¸Šè¯„ä¼°é€‚åº”åçš„æ¨¡å‹
        return self._compute_task_loss(task, adapted_params)
        
    def _compute_meta_gradients(self, task: Task, adapted_params: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—å…ƒæ¢¯åº¦"""
        meta_gradients = {}
        
        for param_name in adapted_params:
            # æ¨¡æ‹Ÿå…ƒæ¢¯åº¦è®¡ç®—
            meta_gradients[param_name] = random.uniform(-0.05, 0.05)
            
        return meta_gradients
        
    def fast_adapt(self, new_task: Task, adaptation_steps: int = 5) -> Dict[str, Any]:
        """å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡"""
        logger.info(f"MAMLå¿«é€Ÿé€‚åº”ä»»åŠ¡: {new_task.task_id}")
        
        # ä»å…ƒå‚æ•°å¼€å§‹é€‚åº”
        adapted_params = copy.deepcopy(self.meta_parameters)
        adaptation_curve = []
        
        for step in range(adaptation_steps):
            # è®¡ç®—å½“å‰æ€§èƒ½
            current_loss = self._compute_task_loss(new_task, adapted_params)
            adaptation_curve.append(current_loss)
            
            # æ¢¯åº¦æ›´æ–°
            gradients = self._compute_task_gradients(new_task, adapted_params)
            for param_name, grad in gradients.items():
                adapted_params[param_name] -= self.learning_rate * grad
                
        final_performance = self.evaluate_adaptation(new_task, adapted_params)
        
        adaptation_result = {
            'adapted_parameters': adapted_params,
            'adaptation_curve': adaptation_curve,
            'final_performance': final_performance,
            'adaptation_steps': adaptation_steps,
            'strategy': MetaLearningStrategy.MODEL_AGNOSTIC
        }
        
        self.adaptation_history.append(adaptation_result)
        logger.info(f"å¿«é€Ÿé€‚åº”å®Œæˆï¼Œæœ€ç»ˆæ€§èƒ½: {final_performance:.4f}")
        
        return adaptation_result
        
    def evaluate_adaptation(self, task: Task, adapted_model: Any) -> float:
        """è¯„ä¼°é€‚åº”æ•ˆæœ"""
        loss = self._compute_task_loss(task, adapted_model)
        # è½¬æ¢ä¸ºæ€§èƒ½åˆ†æ•°ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
        performance = max(0, 1.0 - loss)
        return performance

class PrototypeNetwork(MetaLearningAlgorithm):
    """åŸå‹ç½‘ç»œå…ƒå­¦ä¹ """
    
    def __init__(self, embedding_dim: int = 128):
        self.embedding_dim = embedding_dim
        self.prototype_embeddings = {}
        self.class_prototypes = {}
        
    def meta_train(self, tasks: List[Task]) -> Dict[str, Any]:
        """åŸå‹ç½‘ç»œå…ƒè®­ç»ƒ"""
        logger.info(f"å¼€å§‹åŸå‹ç½‘ç»œå…ƒè®­ç»ƒï¼Œä»»åŠ¡æ•°é‡: {len(tasks)}")
        
        total_accuracy = 0.0
        
        for task in tasks:
            # ä¸ºæ¯ä¸ªç±»è®¡ç®—åŸå‹
            class_prototypes = self._compute_prototypes(task.support_set)
            
            # åœ¨æŸ¥è¯¢é›†ä¸Šè¯„ä¼°
            accuracy = self._evaluate_prototypes(task.query_set, class_prototypes)
            total_accuracy += accuracy
            
            # å­˜å‚¨åŸå‹ä¿¡æ¯
            self.class_prototypes[task.task_id] = class_prototypes
            
        avg_accuracy = total_accuracy / len(tasks)
        
        training_result = {
            'average_accuracy': avg_accuracy,
            'prototypes_learned': len(self.class_prototypes),
            'embedding_dim': self.embedding_dim,
            'strategy': MetaLearningStrategy.PROTOTYPE_BASED
        }
        
        logger.info(f"åŸå‹ç½‘ç»œå…ƒè®­ç»ƒå®Œæˆï¼Œå¹³å‡å‡†ç¡®ç‡: {avg_accuracy:.4f}")
        return training_result
        
    def _compute_prototypes(self, support_set: List[Tuple[Any, Any]]) -> Dict[Any, List[float]]:
        """è®¡ç®—ç±»åŸå‹"""
        class_embeddings = defaultdict(list)
        
        # å°†æ”¯æŒé›†æŒ‰ç±»åˆ«åˆ†ç»„
        for input_data, label in support_set:
            embedding = self._embed_input(input_data)
            class_embeddings[label].append(embedding)
            
        # è®¡ç®—æ¯ä¸ªç±»çš„åŸå‹ï¼ˆå¹³å‡åµŒå…¥ï¼‰
        prototypes = {}
        for class_label, embeddings in class_embeddings.items():
            if embeddings:
                prototype = [sum(dim_values) / len(embeddings) 
                           for dim_values in zip(*embeddings)]
                prototypes[class_label] = prototype
                
        return prototypes
        
    def _embed_input(self, input_data: Any) -> List[float]:
        """å°†è¾“å…¥æ•°æ®åµŒå…¥åˆ°ç‰¹å¾ç©ºé—´"""
        # ç®€åŒ–çš„åµŒå…¥å‡½æ•°
        if isinstance(input_data, str):
            # æ–‡æœ¬åµŒå…¥
            hash_value = hash(input_data)
            embedding = [(hash_value >> i) & 1 for i in range(self.embedding_dim)]
        elif isinstance(input_data, (int, float)):
            # æ•°å€¼åµŒå…¥
            embedding = [math.sin(input_data * i) for i in range(self.embedding_dim)]
        else:
            # é»˜è®¤éšæœºåµŒå…¥
            embedding = [random.random() for _ in range(self.embedding_dim)]
            
        return embedding
        
    def _evaluate_prototypes(self, query_set: List[Tuple[Any, Any]], 
                           prototypes: Dict[Any, List[float]]) -> float:
        """åœ¨æŸ¥è¯¢é›†ä¸Šè¯„ä¼°åŸå‹"""
        if not query_set or not prototypes:
            return 0.0
            
        correct_predictions = 0
        
        for input_data, true_label in query_set:
            query_embedding = self._embed_input(input_data)
            
            # æ‰¾åˆ°æœ€è¿‘çš„åŸå‹
            min_distance = float('inf')
            predicted_label = None
            
            for class_label, prototype in prototypes.items():
                distance = self._euclidean_distance(query_embedding, prototype)
                if distance < min_distance:
                    min_distance = distance
                    predicted_label = class_label
                    
            if predicted_label == true_label:
                correct_predictions += 1
                
        accuracy = correct_predictions / len(query_set)
        return accuracy
        
    def _euclidean_distance(self, vec1: List[float], vec2: List[float]) -> float:
        """è®¡ç®—æ¬§å‡ é‡Œå¾—è·ç¦»"""
        if len(vec1) != len(vec2):
            return float('inf')
            
        distance = sum((a - b) ** 2 for a, b in zip(vec1, vec2))
        return math.sqrt(distance)
        
    def fast_adapt(self, new_task: Task, adaptation_steps: int = 1) -> Dict[str, Any]:
        """åŸå‹ç½‘ç»œå¿«é€Ÿé€‚åº”"""
        logger.info(f"åŸå‹ç½‘ç»œå¿«é€Ÿé€‚åº”ä»»åŠ¡: {new_task.task_id}")
        
        # åŸå‹ç½‘ç»œé€šå¸¸åªéœ€è¦ä¸€æ­¥é€‚åº”
        prototypes = self._compute_prototypes(new_task.support_set)
        performance = self._evaluate_prototypes(new_task.query_set, prototypes)
        
        adaptation_result = {
            'prototypes': prototypes,
            'performance': performance,
            'adaptation_steps': 1,
            'strategy': MetaLearningStrategy.PROTOTYPE_BASED
        }
        
        logger.info(f"åŸå‹ç½‘ç»œé€‚åº”å®Œæˆï¼Œæ€§èƒ½: {performance:.4f}")
        return adaptation_result
        
    def evaluate_adaptation(self, task: Task, adapted_model: Any) -> float:
        """è¯„ä¼°é€‚åº”æ•ˆæœ"""
        if isinstance(adapted_model, dict) and 'prototypes' in adapted_model:
            return self._evaluate_prototypes(task.query_set, adapted_model['prototypes'])
        return 0.0

class MemoryAugmentedNetwork(MetaLearningAlgorithm):
    """è®°å¿†å¢å¼ºç½‘ç»œ"""
    
    def __init__(self, memory_size: int = 1000, memory_dim: int = 128):
        self.memory_size = memory_size
        self.memory_dim = memory_dim
        self.external_memory = {}
        self.memory_usage_count = defaultdict(int)
        
    def meta_train(self, tasks: List[Task]) -> Dict[str, Any]:
        """è®°å¿†å¢å¼ºç½‘ç»œå…ƒè®­ç»ƒ"""
        logger.info(f"å¼€å§‹è®°å¿†å¢å¼ºç½‘ç»œå…ƒè®­ç»ƒï¼Œä»»åŠ¡æ•°é‡: {len(tasks)}")
        
        memories_stored = 0
        
        for task in tasks:
            # ä»ä»»åŠ¡ä¸­æå–å¹¶å­˜å‚¨è®°å¿†
            task_memories = self._extract_task_memories(task)
            
            for memory_key, memory_value in task_memories.items():
                if len(self.external_memory) < self.memory_size:
                    self.external_memory[memory_key] = memory_value
                    memories_stored += 1
                else:
                    # è®°å¿†å·²æ»¡ï¼Œéœ€è¦æ›¿æ¢ç­–ç•¥
                    self._replace_memory(memory_key, memory_value)
                    
        training_result = {
            'memories_stored': memories_stored,
            'memory_utilization': len(self.external_memory) / self.memory_size,
            'strategy': MetaLearningStrategy.MEMORY_AUGMENTED
        }
        
        logger.info(f"è®°å¿†å¢å¼ºç½‘ç»œå…ƒè®­ç»ƒå®Œæˆï¼Œå­˜å‚¨è®°å¿†: {memories_stored}")
        return training_result
        
    def _extract_task_memories(self, task: Task) -> Dict[str, Any]:
        """ä»ä»»åŠ¡ä¸­æå–è®°å¿†"""
        memories = {}
        
        # ä»æ”¯æŒé›†ä¸­æå–æ¨¡å¼
        for i, (input_data, output_data) in enumerate(task.support_set):
            memory_key = f"{task.task_id}_pattern_{i}"
            memory_value = {
                'input_pattern': self._encode_pattern(input_data),
                'output_pattern': self._encode_pattern(output_data),
                'task_context': task.domain,
                'difficulty': task.difficulty,
                'timestamp': time.time()
            }
            memories[memory_key] = memory_value
            
        return memories
        
    def _encode_pattern(self, data: Any) -> List[float]:
        """ç¼–ç æ•°æ®æ¨¡å¼"""
        if isinstance(data, str):
            # æ–‡æœ¬æ¨¡å¼ç¼–ç 
            pattern = [hash(data) % 256 / 255.0]
            pattern.extend([len(data) / 100.0])  # é•¿åº¦ç‰¹å¾
        elif isinstance(data, (int, float)):
            # æ•°å€¼æ¨¡å¼ç¼–ç 
            pattern = [data % 1.0, math.log(abs(data) + 1) / 10.0]
        else:
            # é»˜è®¤æ¨¡å¼
            pattern = [random.random()]
            
        # æ‰©å±•åˆ°å›ºå®šç»´åº¦
        while len(pattern) < self.memory_dim:
            pattern.append(0.0)
            
        return pattern[:self.memory_dim]
        
    def _replace_memory(self, new_key: str, new_value: Any):
        """è®°å¿†æ›¿æ¢ç­–ç•¥"""
        # æ‰¾åˆ°ä½¿ç”¨æ¬¡æ•°æœ€å°‘çš„è®°å¿†è¿›è¡Œæ›¿æ¢
        least_used_key = min(self.external_memory.keys(), 
                           key=lambda k: self.memory_usage_count[k])
        
        del self.external_memory[least_used_key]
        del self.memory_usage_count[least_used_key]
        
        self.external_memory[new_key] = new_value
        
    def fast_adapt(self, new_task: Task, adaptation_steps: int = 3) -> Dict[str, Any]:
        """è®°å¿†å¢å¼ºç½‘ç»œå¿«é€Ÿé€‚åº”"""
        logger.info(f"è®°å¿†å¢å¼ºç½‘ç»œå¿«é€Ÿé€‚åº”ä»»åŠ¡: {new_task.task_id}")
        
        # æ£€ç´¢ç›¸å…³è®°å¿†
        relevant_memories = self._retrieve_relevant_memories(new_task)
        
        # åŸºäºè®°å¿†è¿›è¡Œé€‚åº”
        adaptation_strategy = self._generate_adaptation_strategy(relevant_memories)
        
        # è¯„ä¼°é€‚åº”æ•ˆæœ
        performance = self._evaluate_with_memory(new_task, relevant_memories)
        
        adaptation_result = {
            'relevant_memories': len(relevant_memories),
            'adaptation_strategy': adaptation_strategy,
            'performance': performance,
            'adaptation_steps': adaptation_steps,
            'strategy': MetaLearningStrategy.MEMORY_AUGMENTED
        }
        
        logger.info(f"è®°å¿†å¢å¼ºé€‚åº”å®Œæˆï¼Œä½¿ç”¨è®°å¿†: {len(relevant_memories)}, æ€§èƒ½: {performance:.4f}")
        return adaptation_result
        
    def _retrieve_relevant_memories(self, task: Task) -> List[Dict[str, Any]]:
        """æ£€ç´¢ç›¸å…³è®°å¿†"""
        relevant_memories = []
        task_patterns = [self._encode_pattern(inp) for inp, _ in task.support_set]
        
        for memory_key, memory_value in self.external_memory.items():
            # è®¡ç®—è®°å¿†ä¸ä»»åŠ¡çš„ç›¸å…³æ€§
            memory_pattern = memory_value['input_pattern']
            max_similarity = 0.0
            
            for task_pattern in task_patterns:
                similarity = self._compute_pattern_similarity(memory_pattern, task_pattern)
                max_similarity = max(max_similarity, similarity)
                
            # å¦‚æœç›¸å…³æ€§è¶³å¤Ÿé«˜ï¼Œæ·»åŠ åˆ°ç›¸å…³è®°å¿†
            if max_similarity > 0.7:
                memory_value['similarity'] = max_similarity
                relevant_memories.append(memory_value)
                self.memory_usage_count[memory_key] += 1
                
        # æŒ‰ç›¸å…³æ€§æ’åº
        relevant_memories.sort(key=lambda x: x['similarity'], reverse=True)
        return relevant_memories[:10]  # è¿”å›æœ€ç›¸å…³çš„10ä¸ªè®°å¿†
        
    def _compute_pattern_similarity(self, pattern1: List[float], pattern2: List[float]) -> float:
        """è®¡ç®—æ¨¡å¼ç›¸ä¼¼æ€§"""
        if len(pattern1) != len(pattern2):
            return 0.0
            
        # ä½™å¼¦ç›¸ä¼¼æ€§
        dot_product = sum(a * b for a, b in zip(pattern1, pattern2))
        norm1 = math.sqrt(sum(a * a for a in pattern1))
        norm2 = math.sqrt(sum(b * b for b in pattern2))
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
            
        return dot_product / (norm1 * norm2)
        
    def _generate_adaptation_strategy(self, memories: List[Dict[str, Any]]) -> str:
        """åŸºäºè®°å¿†ç”Ÿæˆé€‚åº”ç­–ç•¥"""
        if not memories:
            return "default_strategy"
            
        # åŸºäºè®°å¿†çš„éš¾åº¦å’Œä¸Šä¸‹æ–‡ç”Ÿæˆç­–ç•¥
        avg_difficulty = sum(m.get('difficulty', 0.5) for m in memories) / len(memories)
        
        if avg_difficulty > 0.7:
            return "conservative_adaptation"
        elif avg_difficulty < 0.3:
            return "aggressive_adaptation"
        else:
            return "balanced_adaptation"
            
    def _evaluate_with_memory(self, task: Task, memories: List[Dict[str, Any]]) -> float:
        """åŸºäºè®°å¿†è¯„ä¼°æ€§èƒ½"""
        if not memories:
            return 0.5  # é»˜è®¤æ€§èƒ½
            
        # åŸºäºè®°å¿†ç›¸å…³æ€§è®¡ç®—æ€§èƒ½
        avg_similarity = sum(m.get('similarity', 0.0) for m in memories) / len(memories)
        
        # æ€§èƒ½ä¸è®°å¿†ç›¸å…³æ€§å’Œä»»åŠ¡å¤æ‚åº¦ç›¸å…³
        base_performance = avg_similarity
        difficulty_penalty = task.difficulty * 0.2
        
        performance = max(0.0, base_performance - difficulty_penalty)
        return min(1.0, performance)
        
    def evaluate_adaptation(self, task: Task, adapted_model: Any) -> float:
        """è¯„ä¼°é€‚åº”æ•ˆæœ"""
        if isinstance(adapted_model, dict) and 'performance' in adapted_model:
            return adapted_model['performance']
        return 0.5

class MetaLearningFramework:
    """å…ƒå­¦ä¹ æ¡†æ¶ä¸»æ§åˆ¶å™¨"""
    
    def __init__(self):
        self.algorithms = {
            MetaLearningStrategy.MODEL_AGNOSTIC: ModelAgnosticMetaLearning(),
            MetaLearningStrategy.PROTOTYPE_BASED: PrototypeNetwork(),
            MetaLearningStrategy.MEMORY_AUGMENTED: MemoryAugmentedNetwork()
        }
        
        self.task_similarity_measurer = TaskSimilarityMeasurer()
        self.meta_knowledge_base = {}
        self.learning_experiences = []
        self.task_history = []
        
        # æ€§èƒ½ç»Ÿè®¡
        self.performance_metrics = {
            'total_adaptations': 0,
            'successful_adaptations': 0,
            'avg_adaptation_performance': 0.0,
            'strategy_performance': defaultdict(list),
            'adaptation_times': defaultdict(list)
        }
        
    def meta_train_all_algorithms(self, training_tasks: List[Task]) -> Dict[str, Any]:
        """å¯¹æ‰€æœ‰ç®—æ³•è¿›è¡Œå…ƒè®­ç»ƒ"""
        logger.info(f"å¼€å§‹å…ƒè®­ç»ƒæ‰€æœ‰ç®—æ³•ï¼Œè®­ç»ƒä»»åŠ¡æ•°: {len(training_tasks)}")
        
        training_results = {}
        
        for strategy, algorithm in self.algorithms.items():
            start_time = time.time()
            
            try:
                result = algorithm.meta_train(training_tasks)
                result['training_time'] = time.time() - start_time
                training_results[strategy.value] = result
                
                logger.info(f"{strategy.value}å…ƒè®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {result['training_time']:.2f}ç§’")
                
            except Exception as e:
                logger.error(f"{strategy.value}å…ƒè®­ç»ƒå¤±è´¥: {str(e)}")
                training_results[strategy.value] = {'error': str(e)}
                
        return training_results
        
    def select_best_strategy(self, new_task: Task) -> MetaLearningStrategy:
        """é€‰æ‹©æœ€ä½³çš„å…ƒå­¦ä¹ ç­–ç•¥"""
        if not self.learning_experiences:
            # æ²¡æœ‰ç»éªŒæ—¶ï¼Œé»˜è®¤ä½¿ç”¨MAML
            return MetaLearningStrategy.MODEL_AGNOSTIC
            
        # æ‰¾åˆ°ç›¸ä¼¼ä»»åŠ¡çš„å†å²ç»éªŒ
        similar_tasks = self.task_similarity_measurer.find_similar_tasks(
            new_task, self.task_history, top_k=5
        )
        
        if not similar_tasks:
            return MetaLearningStrategy.MODEL_AGNOSTIC
            
        # ç»Ÿè®¡å„ç­–ç•¥åœ¨ç›¸ä¼¼ä»»åŠ¡ä¸Šçš„è¡¨ç°
        strategy_scores = defaultdict(list)
        
        for experience in self.learning_experiences:
            for similar_task, similarity in similar_tasks:
                if experience.task_id == similar_task.task_id:
                    weighted_performance = experience.performance * similarity
                    strategy_scores[experience.strategy_used].append(weighted_performance)
                    
        # é€‰æ‹©å¹³å‡è¡¨ç°æœ€å¥½çš„ç­–ç•¥
        best_strategy = MetaLearningStrategy.MODEL_AGNOSTIC
        best_score = 0.0
        
        for strategy, scores in strategy_scores.items():
            if scores:
                avg_score = sum(scores) / len(scores)
                if avg_score > best_score:
                    best_score = avg_score
                    best_strategy = strategy
                    
        logger.info(f"ä¸ºä»»åŠ¡ {new_task.task_id} é€‰æ‹©ç­–ç•¥: {best_strategy.value} (é¢„æœŸæ€§èƒ½: {best_score:.3f})")
        return best_strategy
        
    def fast_adapt_to_new_task(self, new_task: Task, strategy: Optional[MetaLearningStrategy] = None,
                              adaptation_steps: int = 5) -> Dict[str, Any]:
        """å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡"""
        if strategy is None:
            strategy = self.select_best_strategy(new_task)
            
        logger.info(f"å¼€å§‹å¿«é€Ÿé€‚åº”ä»»åŠ¡: {new_task.task_id}ï¼Œä½¿ç”¨ç­–ç•¥: {strategy.value}")
        
        start_time = time.time()
        
        try:
            algorithm = self.algorithms[strategy]
            adaptation_result = algorithm.fast_adapt(new_task, adaptation_steps)
            
            adaptation_time = time.time() - start_time
            adaptation_result['adaptation_time'] = adaptation_time
            adaptation_result['selected_strategy'] = strategy.value
            
            # è®°å½•å­¦ä¹ ç»éªŒ
            experience = LearningExperience(
                task_id=new_task.task_id,
                strategy_used=strategy,
                performance=adaptation_result.get('performance', adaptation_result.get('final_performance', 0.5)),
                adaptation_steps=adaptation_steps,
                learning_curve=adaptation_result.get('adaptation_curve', []),
                insights=self._extract_insights(adaptation_result)
            )
            
            self.learning_experiences.append(experience)
            self.task_history.append(new_task)
            
            # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
            self._update_performance_metrics(strategy, experience, adaptation_time)
            
            logger.info(f"å¿«é€Ÿé€‚åº”å®Œæˆï¼Œæ€§èƒ½: {experience.performance:.4f}ï¼Œè€—æ—¶: {adaptation_time:.3f}ç§’")
            
            return adaptation_result
            
        except Exception as e:
            logger.error(f"å¿«é€Ÿé€‚åº”å¤±è´¥: {str(e)}")
            return {'error': str(e), 'strategy': strategy.value}
            
    def _extract_insights(self, adaptation_result: Dict[str, Any]) -> List[str]:
        """ä»é€‚åº”ç»“æœä¸­æå–æ´å¯Ÿ"""
        insights = []
        
        performance = adaptation_result.get('performance', adaptation_result.get('final_performance', 0))
        
        if performance > 0.8:
            insights.append("é«˜æ€§èƒ½é€‚åº”ï¼Œç­–ç•¥é€‰æ‹©æ­£ç¡®")
        elif performance < 0.4:
            insights.append("é€‚åº”æ€§èƒ½è¾ƒä½ï¼Œå¯èƒ½éœ€è¦æ›´å¤šé€‚åº”æ­¥éª¤æˆ–ä¸åŒç­–ç•¥")
            
        adaptation_curve = adaptation_result.get('adaptation_curve', [])
        if len(adaptation_curve) > 1:
            if adaptation_curve[-1] < adaptation_curve[0]:
                insights.append("å­¦ä¹ æ›²çº¿æ˜¾ç¤ºæŒç»­æ”¹è¿›")
            else:
                insights.append("å¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆæˆ–æ”¶æ•›é—®é¢˜")
                
        return insights
        
    def _update_performance_metrics(self, strategy: MetaLearningStrategy, 
                                  experience: LearningExperience, adaptation_time: float):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
        self.performance_metrics['total_adaptations'] += 1
        
        if experience.performance > 0.6:  # è®¤ä¸ºæ€§èƒ½>0.6ä¸ºæˆåŠŸ
            self.performance_metrics['successful_adaptations'] += 1
            
        # æ›´æ–°å¹³å‡æ€§èƒ½
        total_adaptations = self.performance_metrics['total_adaptations']
        current_avg = self.performance_metrics['avg_adaptation_performance']
        new_avg = (current_avg * (total_adaptations - 1) + experience.performance) / total_adaptations
        self.performance_metrics['avg_adaptation_performance'] = new_avg
        
        # è®°å½•ç­–ç•¥ç‰¹å®šæ€§èƒ½
        self.performance_metrics['strategy_performance'][strategy.value].append(experience.performance)
        self.performance_metrics['adaptation_times'][strategy.value].append(adaptation_time)
        
    def get_performance_report(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        success_rate = (
            self.performance_metrics['successful_adaptations'] / 
            max(self.performance_metrics['total_adaptations'], 1)
        )
        
        strategy_stats = {}
        for strategy, performances in self.performance_metrics['strategy_performance'].items():
            if performances:
                strategy_stats[strategy] = {
                    'avg_performance': sum(performances) / len(performances),
                    'best_performance': max(performances),
                    'usage_count': len(performances),
                    'avg_time': sum(self.performance_metrics['adaptation_times'][strategy]) / len(performances)
                }
                
        return {
            "æ€»é€‚åº”æ¬¡æ•°": self.performance_metrics['total_adaptations'],
            "æˆåŠŸé€‚åº”æ¬¡æ•°": self.performance_metrics['successful_adaptations'],
            "æˆåŠŸç‡": f"{success_rate:.2%}",
            "å¹³å‡é€‚åº”æ€§èƒ½": f"{self.performance_metrics['avg_adaptation_performance']:.3f}",
            "ç­–ç•¥ç»Ÿè®¡": strategy_stats,
            "å­¦ä¹ ç»éªŒæ•°": len(self.learning_experiences),
            "ä»»åŠ¡å†å²æ•°": len(self.task_history)
        }
        
    def generate_meta_insights(self) -> List[str]:
        """ç”Ÿæˆå…ƒå­¦ä¹ æ´å¯Ÿ"""
        insights = []
        
        if len(self.learning_experiences) < 5:
            insights.append("ç»éªŒæ•°æ®ä¸è¶³ï¼Œå»ºè®®è¿›è¡Œæ›´å¤šå…ƒè®­ç»ƒ")
            return insights
            
        # åˆ†æç­–ç•¥åå¥½
        strategy_usage = defaultdict(int)
        for exp in self.learning_experiences:
            strategy_usage[exp.strategy_used.value] += 1
            
        most_used_strategy = max(strategy_usage.items(), key=lambda x: x[1])
        insights.append(f"æœ€å¸¸ç”¨ç­–ç•¥: {most_used_strategy[0]} (ä½¿ç”¨{most_used_strategy[1]}æ¬¡)")
        
        # åˆ†ææ€§èƒ½è¶‹åŠ¿
        recent_performances = [exp.performance for exp in self.learning_experiences[-10:]]
        if len(recent_performances) >= 5:
            trend = sum(recent_performances[-3:]) / 3 - sum(recent_performances[:3]) / 3
            if trend > 0.05:
                insights.append("è¿‘æœŸé€‚åº”æ€§èƒ½å‘ˆä¸Šå‡è¶‹åŠ¿")
            elif trend < -0.05:
                insights.append("è¿‘æœŸé€‚åº”æ€§èƒ½æœ‰æ‰€ä¸‹é™ï¼Œå»ºè®®åˆ†æåŸå› ")
                
        # åˆ†æä»»åŠ¡ç‰¹æ€§
        task_types = [task.task_type for task in self.task_history]
        if task_types:
            most_common_type = max(set(task_types), key=task_types.count)
            insights.append(f"æœ€å¸¸å¤„ç†çš„ä»»åŠ¡ç±»å‹: {most_common_type.value}")
            
        return insights

# ç¤ºä¾‹ä½¿ç”¨å’Œæµ‹è¯•
async def demonstrate_meta_learning():
    """æ¼”ç¤ºå…ƒå­¦ä¹ æ¡†æ¶åŠŸèƒ½"""
    print("ğŸ§  è‡ªä¸»è¿›åŒ–Agent - ç¬¬3è½®æå‡ï¼šå…ƒå­¦ä¹ æ¡†æ¶")
    print("=" * 60)
    
    # åˆ›å»ºå…ƒå­¦ä¹ æ¡†æ¶
    meta_framework = MetaLearningFramework()
    
    # ç”Ÿæˆè®­ç»ƒä»»åŠ¡
    print("\nğŸ“š ç”Ÿæˆå…ƒè®­ç»ƒä»»åŠ¡")
    training_tasks = []
    
    for i in range(5):
        task = Task(
            task_id=f"train_task_{i}",
            task_type=TaskType.CLASSIFICATION,
            domain=f"domain_{i % 3}",
            description=f"åˆ†ç±»ä»»åŠ¡{i}ï¼šè¯†åˆ«æ•°æ®æ¨¡å¼",
            support_set=[(f"input_{j}", f"label_{j % 2}") for j in range(10)],
            query_set=[(f"query_{j}", f"label_{j % 2}") for j in range(5)],
            difficulty=random.uniform(0.3, 0.8)
        )
        training_tasks.append(task)
        
    # å…ƒè®­ç»ƒ
    print("\nğŸ”§ å¼€å§‹å…ƒè®­ç»ƒ")
    training_results = meta_framework.meta_train_all_algorithms(training_tasks)
    
    for strategy, result in training_results.items():
        if 'error' not in result:
            print(f"  {strategy}: è®­ç»ƒæ—¶é—´ {result.get('training_time', 0):.2f}ç§’")
        else:
            print(f"  {strategy}: è®­ç»ƒå¤±è´¥ - {result['error']}")
            
    # å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡
    print("\nâš¡ å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡")
    new_tasks = [
        Task(
            task_id="new_task_1",
            task_type=TaskType.CLASSIFICATION,
            domain="domain_1",
            description="æ–°çš„åˆ†ç±»ä»»åŠ¡ï¼šæ–‡æœ¬æƒ…æ„Ÿåˆ†æ",
            support_set=[("æ­£é¢æ–‡æœ¬", "positive"), ("è´Ÿé¢æ–‡æœ¬", "negative")],
            query_set=[("æµ‹è¯•æ–‡æœ¬", "positive")],
            difficulty=0.6
        ),
        Task(
            task_id="new_task_2",
            task_type=TaskType.REGRESSION,
            domain="domain_2",
            description="å›å½’ä»»åŠ¡ï¼šæ•°å€¼é¢„æµ‹",
            support_set=[(1.0, 2.0), (2.0, 4.0), (3.0, 6.0)],
            query_set=[(4.0, 8.0)],
            difficulty=0.4
        )
    ]
    
    for new_task in new_tasks:
        print(f"\nğŸ¯ é€‚åº”ä»»åŠ¡: {new_task.task_id}")
        adaptation_result = meta_framework.fast_adapt_to_new_task(new_task)
        
        if 'error' not in adaptation_result:
            performance = adaptation_result.get('performance', adaptation_result.get('final_performance', 0))
            strategy = adaptation_result.get('selected_strategy', 'unknown')
            time_cost = adaptation_result.get('adaptation_time', 0)
            
            print(f"  ç­–ç•¥: {strategy}")
            print(f"  æ€§èƒ½: {performance:.3f}")
            print(f"  è€—æ—¶: {time_cost:.3f}ç§’")
        else:
            print(f"  é€‚åº”å¤±è´¥: {adaptation_result['error']}")
            
    # æ€§èƒ½æŠ¥å‘Š
    print("\nğŸ“Š å…ƒå­¦ä¹ æ€§èƒ½æŠ¥å‘Š")
    report = meta_framework.get_performance_report()
    for key, value in report.items():
        if isinstance(value, dict):
            print(f"{key}:")
            for sub_key, sub_value in value.items():
                print(f"  {sub_key}: {sub_value}")
        else:
            print(f"{key}: {value}")
            
    # å…ƒå­¦ä¹ æ´å¯Ÿ
    print("\nğŸ’¡ å…ƒå­¦ä¹ æ´å¯Ÿ")
    insights = meta_framework.generate_meta_insights()
    for insight in insights:
        print(f"  â€¢ {insight}")
        
    print("\nâœ… ç¬¬3è½®æå‡å®Œæˆï¼å…ƒå­¦ä¹ æ¡†æ¶å·²æˆåŠŸéƒ¨ç½²")

if __name__ == "__main__":
    asyncio.run(demonstrate_meta_learning())