#!/usr/bin/env python3
"""
è¯„ä¼°ç³»ç»Ÿæµ‹è¯•æ¨¡å— - è‡ªä¸»è¿›åŒ–Agentç³»ç»Ÿ
æµ‹è¯•9ç»´åº¦è¯„ä¼°ç³»ç»Ÿçš„åŠŸèƒ½
"""

import unittest
import asyncio
import sys
import os

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from autonomous_evolutionary_agent_system import (
    AutonomousEvolutionarySystem,
    PerformanceMetrics,
    TrainingFreeEvaluator
)


class TestPerformanceMetrics(unittest.TestCase):
    """æµ‹è¯•æ€§èƒ½æŒ‡æ ‡ç±»"""
    
    def setUp(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        self.metrics = PerformanceMetrics(
            trainability=0.8,
            generalization=0.7,
            expressiveness=0.6,
            creativity_score=0.9,
            adaptation_rate=0.75,
            collaboration_efficiency=0.85,
            error_recovery_rate=0.8,
            knowledge_retention=0.7,
            innovation_index=0.65
        )
    
    def test_metrics_initialization(self):
        """æµ‹è¯•æŒ‡æ ‡åˆå§‹åŒ–"""
        self.assertEqual(self.metrics.trainability, 0.8)
        self.assertEqual(self.metrics.generalization, 0.7)
        self.assertEqual(self.metrics.creativity_score, 0.9)
        self.assertGreater(self.metrics.composite_score, 0)
        self.assertLessEqual(self.metrics.composite_score, 1)
    
    def test_composite_score_calculation(self):
        """æµ‹è¯•ç»¼åˆå¾—åˆ†è®¡ç®—"""
        # ç»¼åˆå¾—åˆ†åº”è¯¥æ˜¯åŠ æƒå¹³å‡
        expected_score = (
            0.8 * 0.15 +  # trainability
            0.7 * 0.15 +  # generalization
            0.6 * 0.10 +  # expressiveness
            0.9 * 0.15 +  # creativity_score
            0.75 * 0.10 + # adaptation_rate
            0.85 * 0.15 + # collaboration_efficiency
            0.8 * 0.10 +  # error_recovery_rate
            0.7 * 0.05 +  # knowledge_retention
            0.65 * 0.05   # innovation_index
        )
        
        self.assertAlmostEqual(self.metrics.composite_score, expected_score, places=3)
    
    def test_to_dict(self):
        """æµ‹è¯•è½¬æ¢ä¸ºå­—å…¸"""
        metrics_dict = self.metrics.to_dict()
        
        self.assertIsInstance(metrics_dict, dict)
        self.assertIn('trainability', metrics_dict)
        self.assertIn('composite_score', metrics_dict)
        self.assertEqual(len(metrics_dict), 10)  # 9ä¸ªç»´åº¦ + ç»¼åˆå¾—åˆ†
    
    def test_from_dict(self):
        """æµ‹è¯•ä»å­—å…¸åˆ›å»º"""
        data = {
            'trainability': 0.8,
            'generalization': 0.7,
            'expressiveness': 0.6,
            'creativity_score': 0.9,
            'adaptation_rate': 0.75,
            'collaboration_efficiency': 0.85,
            'error_recovery_rate': 0.8,
            'knowledge_retention': 0.7,
            'innovation_index': 0.65
        }
        
        new_metrics = PerformanceMetrics.from_dict(data)
        
        self.assertEqual(new_metrics.trainability, 0.8)
        self.assertEqual(new_metrics.creativity_score, 0.9)
    
    def test_metrics_bounds(self):
        """æµ‹è¯•æŒ‡æ ‡è¾¹ç•Œå€¼"""
        # æµ‹è¯•æœ€å°å€¼
        min_metrics = PerformanceMetrics(
            trainability=0.0,
            generalization=0.0,
            expressiveness=0.0,
            creativity_score=0.0,
            adaptation_rate=0.0,
            collaboration_efficiency=0.0,
            error_recovery_rate=0.0,
            knowledge_retention=0.0,
            innovation_index=0.0
        )
        
        self.assertEqual(min_metrics.composite_score, 0.0)
        
        # æµ‹è¯•æœ€å¤§å€¼
        max_metrics = PerformanceMetrics(
            trainability=1.0,
            generalization=1.0,
            expressiveness=1.0,
            creativity_score=1.0,
            adaptation_rate=1.0,
            collaboration_efficiency=1.0,
            error_recovery_rate=1.0,
            knowledge_retention=1.0,
            innovation_index=1.0
        )
        
        self.assertEqual(max_metrics.composite_score, 1.0)


class TestTrainingFreeEvaluator(unittest.TestCase):
    """æµ‹è¯•è®­ç»ƒæ— å…³è¯„ä¼°å™¨"""
    
    def setUp(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        self.evaluator = TrainingFreeEvaluator()
    
    def test_evaluator_initialization(self):
        """æµ‹è¯•è¯„ä¼°å™¨åˆå§‹åŒ–"""
        self.assertIsInstance(self.evaluator, TrainingFreeEvaluator)
    
    def test_evaluate_trainability(self):
        """æµ‹è¯•å¯è®­ç»ƒæ€§è¯„ä¼°"""
        # æ¨¡æ‹Ÿæ¨¡å‹å‚æ•°
        model_params = {
            'param_count': 1000000,
            'layer_count': 10,
            'gradient_stats': {'mean': 0.01, 'std': 0.1}
        }
        
        trainability = self.evaluator.evaluate_trainability(model_params)
        
        self.assertIsInstance(trainability, float)
        self.assertGreaterEqual(trainability, 0)
        self.assertLessEqual(trainability, 1)
    
    def test_evaluate_generalization(self):
        """æµ‹è¯•æ³›åŒ–èƒ½åŠ›è¯„ä¼°"""
        model_complexity = 0.5
        
        generalization = self.evaluator.evaluate_generalization(model_complexity)
        
        self.assertIsInstance(generalization, float)
        self.assertGreaterEqual(generalization, 0)
        self.assertLessEqual(generalization, 1)
    
    def test_evaluate_expressiveness(self):
        """æµ‹è¯•è¡¨è¾¾èƒ½åŠ›è¯„ä¼°"""
        architecture_info = {
            'depth': 5,
            'width': 100,
            'activation_diversity': 0.8,
            'connection_density': 0.6
        }
        
        expressiveness = self.evaluator.evaluate_expressiveness(architecture_info)
        
        self.assertIsInstance(expressiveness, float)
        self.assertGreaterEqual(expressiveness, 0)
        self.assertLessEqual(expressiveness, 1)
    
    def test_evaluate_creativity(self):
        """æµ‹è¯•åˆ›é€ æ€§è¯„ä¼°"""
        output_samples = [
            "åˆ›æ–°çš„è§£å†³æ–¹æ¡ˆA",
            "æ ‡å‡†æ–¹æ³•B", 
            "ç‹¬ç‰¹çš„æƒ³æ³•C"
        ]
        
        creativity = self.evaluator.evaluate_creativity(output_samples)
        
        self.assertIsInstance(creativity, float)
        self.assertGreaterEqual(creativity, 0)
        self.assertLessEqual(creativity, 1)
    
    def test_evaluate_adaptation_rate(self):
        """æµ‹è¯•é€‚åº”é€Ÿåº¦è¯„ä¼°"""
        performance_history = [0.3, 0.4, 0.5, 0.6, 0.7]
        
        adaptation_rate = self.evaluator.evaluate_adaptation_rate(performance_history)
        
        self.assertIsInstance(adaptation_rate, float)
        self.assertGreaterEqual(adaptation_rate, 0)
        self.assertLessEqual(adaptation_rate, 1)


class TestSystemEvaluation(unittest.TestCase):
    """æµ‹è¯•ç³»ç»Ÿçº§è¯„ä¼°"""
    
    def setUp(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        self.system = AutonomousEvolutionarySystem()
    
    async def test_system_performance_evaluation(self):
        """æµ‹è¯•ç³»ç»Ÿæ€§èƒ½è¯„ä¼°"""
        # åˆ›å»ºå›¢é˜Ÿ
        team = self.system.create_standard_team()
        
        # è¯„ä¼°ç³»ç»Ÿæ€§èƒ½
        metrics = await self.system.evaluate_system_performance()
        
        # æ£€æŸ¥è¯„ä¼°ç»“æœ
        self.assertIsInstance(metrics, PerformanceMetrics)
        self.assertGreaterEqual(metrics.composite_score, 0)
        self.assertLessEqual(metrics.composite_score, 1)
        
        # æ£€æŸ¥æ‰€æœ‰ç»´åº¦
        self.assertGreaterEqual(metrics.trainability, 0)
        self.assertGreaterEqual(metrics.generalization, 0)
        self.assertGreaterEqual(metrics.creativity_score, 0)
        self.assertGreaterEqual(metrics.collaboration_efficiency, 0)
    
    async def test_evaluation_consistency(self):
        """æµ‹è¯•è¯„ä¼°ä¸€è‡´æ€§"""
        # åˆ›å»ºå›¢é˜Ÿ
        team = self.system.create_standard_team()
        
        # å¤šæ¬¡è¯„ä¼°
        metrics1 = await self.system.evaluate_system_performance()
        metrics2 = await self.system.evaluate_system_performance()
        
        # è¯„ä¼°ç»“æœåº”è¯¥ç›¸å¯¹ç¨³å®š
        score_diff = abs(metrics1.composite_score - metrics2.composite_score)
        self.assertLess(score_diff, 0.2)  # å·®å¼‚ä¸åº”è¶…è¿‡0.2
    
    async def test_evaluation_after_task(self):
        """æµ‹è¯•ä»»åŠ¡åè¯„ä¼°"""
        # åˆ›å»ºå›¢é˜Ÿ
        team = self.system.create_standard_team()
        
        # è¯„ä¼°åˆå§‹æ€§èƒ½
        initial_metrics = await self.system.evaluate_system_performance()
        
        # è¿è¡Œä»»åŠ¡
        await self.system.run_collaborative_task(
            goal="æµ‹è¯•ä»»åŠ¡",
            max_cycles=1
        )
        
        # è¯„ä¼°ä»»åŠ¡åæ€§èƒ½
        final_metrics = await self.system.evaluate_system_performance()
        
        # æ€§èƒ½åº”è¯¥æœ‰æ‰€å˜åŒ–ï¼ˆå­¦ä¹ æ•ˆæœï¼‰
        self.assertIsInstance(initial_metrics, PerformanceMetrics)
        self.assertIsInstance(final_metrics, PerformanceMetrics)


class TestEvaluationMetrics(unittest.TestCase):
    """æµ‹è¯•è¯„ä¼°æŒ‡æ ‡è®¡ç®—"""
    
    def test_collaboration_efficiency_calculation(self):
        """æµ‹è¯•åä½œæ•ˆç‡è®¡ç®—"""
        evaluator = TrainingFreeEvaluator()
        
        # æ¨¡æ‹Ÿåä½œæ•°æ®
        collaboration_data = {
            'message_count': 50,
            'task_completion_rate': 0.8,
            'conflict_resolution_time': 10.0,
            'resource_utilization': 0.75
        }
        
        efficiency = evaluator.evaluate_collaboration_efficiency(collaboration_data)
        
        self.assertIsInstance(efficiency, float)
        self.assertGreaterEqual(efficiency, 0)
        self.assertLessEqual(efficiency, 1)
    
    def test_error_recovery_calculation(self):
        """æµ‹è¯•é”™è¯¯æ¢å¤ç‡è®¡ç®—"""
        evaluator = TrainingFreeEvaluator()
        
        # æ¨¡æ‹Ÿé”™è¯¯æ¢å¤æ•°æ®
        error_data = {
            'total_errors': 10,
            'recovered_errors': 8,
            'recovery_time': [1.0, 2.0, 1.5, 3.0, 2.5, 1.0, 4.0, 2.0],
            'performance_degradation': 0.1
        }
        
        recovery_rate = evaluator.evaluate_error_recovery_rate(error_data)
        
        self.assertIsInstance(recovery_rate, float)
        self.assertGreaterEqual(recovery_rate, 0)
        self.assertLessEqual(recovery_rate, 1)
    
    def test_knowledge_retention_calculation(self):
        """æµ‹è¯•çŸ¥è¯†ä¿æŒç‡è®¡ç®—"""
        evaluator = TrainingFreeEvaluator()
        
        # æ¨¡æ‹ŸçŸ¥è¯†ä¿æŒæ•°æ®
        knowledge_data = {
            'learned_patterns': 20,
            'retained_patterns': 18,
            'memory_usage': 0.8,
            'pattern_accuracy': 0.9
        }
        
        retention_rate = evaluator.evaluate_knowledge_retention(knowledge_data)
        
        self.assertIsInstance(retention_rate, float)
        self.assertGreaterEqual(retention_rate, 0)
        self.assertLessEqual(retention_rate, 1)
    
    def test_innovation_index_calculation(self):
        """æµ‹è¯•åˆ›æ–°æŒ‡æ•°è®¡ç®—"""
        evaluator = TrainingFreeEvaluator()
        
        # æ¨¡æ‹Ÿåˆ›æ–°æ•°æ®
        innovation_data = {
            'novel_solutions': 5,
            'total_solutions': 20,
            'solution_diversity': 0.7,
            'breakthrough_count': 2
        }
        
        innovation_index = evaluator.evaluate_innovation_index(innovation_data)
        
        self.assertIsInstance(innovation_index, float)
        self.assertGreaterEqual(innovation_index, 0)
        self.assertLessEqual(innovation_index, 1)


def run_async_test(test_func):
    """è¿è¡Œå¼‚æ­¥æµ‹è¯•çš„è¾…åŠ©å‡½æ•°"""
    def wrapper(self):
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            loop.run_until_complete(test_func(self))
        finally:
            loop.close()
    return wrapper


# åŒ…è£…å¼‚æ­¥æµ‹è¯•
TestSystemEvaluation.test_system_performance_evaluation = run_async_test(
    TestSystemEvaluation.test_system_performance_evaluation
)
TestSystemEvaluation.test_evaluation_consistency = run_async_test(
    TestSystemEvaluation.test_evaluation_consistency
)
TestSystemEvaluation.test_evaluation_after_task = run_async_test(
    TestSystemEvaluation.test_evaluation_after_task
)


if __name__ == '__main__':
    print("ğŸ§ª è¿è¡Œè¯„ä¼°ç³»ç»Ÿæµ‹è¯•å¥—ä»¶")
    print("=" * 50)
    
    # è¿è¡Œæµ‹è¯•
    unittest.main(verbosity=2)